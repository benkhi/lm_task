nohup: ignoring input
2023-08-25 09:10:36 INFO     INITIALIZE GRID SEARCHER: 12 configs to try
2023-08-25 09:10:36 INFO     ## 1st RUN: Configuration 0/12 ##
2023-08-25 09:10:36 INFO     initialize model trainer
2023-08-25 09:10:36 INFO     initialize checkpoint at tmp_ckpt_t5-base/model_avxiio
2023-08-25 09:10:36 INFO     hyperparameters
2023-08-25 09:10:36 INFO     	 * dataset_path: lmqg/qg_squad
2023-08-25 09:10:36 INFO     	 * dataset_name: default
2023-08-25 09:10:36 INFO     	 * input_types: paragraph_answer
2023-08-25 09:10:36 INFO     	 * output_types: question
2023-08-25 09:10:36 INFO     	 * prefix_types: None
2023-08-25 09:10:36 INFO     	 * model: t5-base
2023-08-25 09:10:36 INFO     	 * max_length: 512
2023-08-25 09:10:36 INFO     	 * max_length_output: 32
2023-08-25 09:10:36 INFO     	 * epoch: 15
2023-08-25 09:10:36 INFO     	 * batch: 64
2023-08-25 09:10:36 INFO     	 * lr: 0.001
2023-08-25 09:10:36 INFO     	 * fp16: False
2023-08-25 09:10:36 INFO     	 * random_seed: 1
2023-08-25 09:10:36 INFO     	 * gradient_accumulation_steps: 4
2023-08-25 09:10:36 INFO     	 * label_smoothing: 0.15
2023-08-25 09:10:36 INFO     initialize checkpoint with t5-base
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 1.43MB/s]Downloading (…)/main/tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 1.42MB/s]
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
2023-08-25 09:10:39 INFO     Created a temporary directory at /tmp/tmp519pc1n3
2023-08-25 09:10:39 INFO     Writing /tmp/tmp519pc1n3/_remote_module_non_scriptable.py
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
2023-08-25 09:10:50 INFO     use spaCy answer extraction model: positionrank
2023-08-25 09:10:53 INFO     Model `t5-base`
2023-08-25 09:10:53 INFO     	 * Num of GPU in use: 2
2023-08-25 09:10:53 INFO     	 * Prefix: False
2023-08-25 09:10:53 INFO     	 * Language: en (ignore at the training phase)
2023-08-25 09:10:53 INFO     dataset preprocessing
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=False' instead.
  warnings.warn(
2023-08-25 09:10:55 INFO     encode all the data       : 75722
  0%|          | 0/75722 [00:00<?, ?it/s]  0%|          | 61/75722 [00:00<02:05, 603.06it/s]  0%|          | 150/75722 [00:00<01:38, 768.66it/s]  0%|          | 236/75722 [00:00<01:33, 809.70it/s]  0%|          | 318/75722 [00:00<01:41, 742.64it/s]  1%|          | 412/75722 [00:00<01:33, 806.65it/s]  1%|          | 508/75722 [00:00<01:28, 853.14it/s]  1%|          | 595/75722 [00:00<01:28, 845.06it/s]  1%|          | 704/75722 [00:00<01:21, 920.44it/s]  1%|          | 797/75722 [00:01<01:43, 721.17it/s]  1%|          | 893/75722 [00:01<01:35, 779.89it/s]  1%|▏         | 986/75722 [00:01<01:31, 818.00it/s]  1%|▏         | 1074/75722 [00:01<01:29, 833.57it/s]  2%|▏         | 1161/75722 [00:01<01:39, 753.03it/s]  2%|▏         | 1240/75722 [00:01<01:39, 749.20it/s]  2%|▏         | 1330/75722 [00:01<01:34, 784.45it/s]  2%|▏         | 1415/75722 [00:01<01:32, 801.83it/s]  2%|▏         | 1513/75722 [00:01<01:27, 850.28it/s]  2%|▏         | 1600/75722 [00:02<01:32, 803.75it/s]  2%|▏         | 1682/75722 [00:02<01:35, 772.18it/s]  2%|▏         | 1761/75722 [00:02<01:41, 731.32it/s]  2%|▏         | 1836/75722 [00:02<01:44, 704.49it/s]  3%|▎         | 1918/75722 [00:02<01:40, 732.99it/s]  3%|▎         | 1993/75722 [00:02<01:41, 723.29it/s]  3%|▎         | 2072/75722 [00:02<01:39, 738.19it/s]  3%|▎         | 2161/75722 [00:02<01:34, 779.50it/s]  3%|▎         | 2244/75722 [00:02<01:32, 790.81it/s]  3%|▎         | 2325/75722 [00:02<01:32, 796.38it/s]  3%|▎         | 2412/75722 [00:03<01:30, 809.83it/s]  3%|▎         | 2500/75722 [00:03<01:28, 828.85it/s]  3%|▎         | 2605/75722 [00:03<01:21, 893.72it/s]  4%|▎         | 2707/75722 [00:03<01:18, 930.82it/s]  4%|▎         | 2801/75722 [00:03<01:20, 901.44it/s]  4%|▍         | 2900/75722 [00:03<01:18, 926.58it/s]  4%|▍         | 2993/75722 [00:03<01:23, 872.21it/s]  4%|▍         | 3082/75722 [00:03<01:29, 810.30it/s]  4%|▍         | 3165/75722 [00:03<01:31, 791.37it/s]  4%|▍         | 3251/75722 [00:04<01:30, 804.10it/s]  4%|▍         | 3349/75722 [00:04<01:24, 852.16it/s]  5%|▍         | 3458/75722 [00:04<01:18, 917.57it/s]  5%|▍         | 3551/75722 [00:04<01:20, 895.22it/s]  5%|▍         | 3642/75722 [00:04<01:20, 890.32it/s]  5%|▍         | 3746/75722 [00:04<01:17, 932.94it/s]  5%|▌         | 3866/75722 [00:04<01:11, 1006.74it/s]  5%|▌         | 3975/75722 [00:04<01:09, 1029.72it/s]  5%|▌         | 4081/75722 [00:04<01:09, 1032.07it/s]  6%|▌         | 4185/75722 [00:04<01:09, 1022.31it/s]  6%|▌         | 4288/75722 [00:05<01:12, 985.10it/s]   6%|▌         | 4387/75722 [00:05<01:16, 929.61it/s]  6%|▌         | 4481/75722 [00:05<01:17, 923.26it/s]  6%|▌         | 4577/75722 [00:05<01:16, 932.91it/s]  6%|▌         | 4677/75722 [00:05<01:14, 950.92it/s]  6%|▋         | 4773/75722 [00:05<01:15, 939.21it/s]  6%|▋         | 4870/75722 [00:05<01:14, 946.95it/s]  7%|▋         | 4965/75722 [00:05<01:20, 884.08it/s]  7%|▋         | 5055/75722 [00:05<01:19, 888.19it/s]  7%|▋         | 5145/75722 [00:06<01:23, 843.87it/s]  7%|▋         | 5242/75722 [00:06<01:20, 876.91it/s]  7%|▋         | 5336/75722 [00:06<01:18, 893.88it/s]  7%|▋         | 5427/75722 [00:06<02:36, 448.24it/s]  7%|▋         | 5517/75722 [00:06<02:13, 525.24it/s]  7%|▋         | 5598/75722 [00:06<02:00, 579.78it/s]  8%|▊         | 5686/75722 [00:07<01:48, 644.40it/s]  8%|▊         | 5774/75722 [00:07<01:39, 699.49it/s]  8%|▊         | 5877/75722 [00:07<01:29, 782.28it/s]  8%|▊         | 5984/75722 [00:07<01:21, 857.40it/s]  8%|▊         | 6090/75722 [00:07<01:16, 910.31it/s]  8%|▊         | 6188/75722 [00:07<01:16, 913.96it/s]  8%|▊         | 6284/75722 [00:07<01:17, 891.87it/s]  8%|▊         | 6377/75722 [00:07<01:20, 858.38it/s]  9%|▊         | 6466/75722 [00:07<01:21, 854.85it/s]  9%|▊         | 6554/75722 [00:07<01:28, 784.03it/s]  9%|▉         | 6635/75722 [00:08<01:27, 788.66it/s]  9%|▉         | 6723/75722 [00:08<01:24, 813.57it/s]  9%|▉         | 6806/75722 [00:08<01:26, 796.82it/s]  9%|▉         | 6901/75722 [00:08<01:21, 839.49it/s]  9%|▉         | 6986/75722 [00:08<01:21, 842.13it/s]  9%|▉         | 7071/75722 [00:08<01:22, 834.48it/s]  9%|▉         | 7169/75722 [00:08<01:18, 874.83it/s] 10%|▉         | 7265/75722 [00:08<01:16, 895.23it/s] 10%|▉         | 7357/75722 [00:08<01:15, 902.42it/s] 10%|▉         | 7450/75722 [00:09<01:15, 908.49it/s] 10%|▉         | 7549/75722 [00:09<01:13, 930.23it/s] 10%|█         | 7661/75722 [00:09<01:09, 983.74it/s] 10%|█         | 7760/75722 [00:09<01:12, 932.48it/s] 10%|█         | 7854/75722 [00:09<01:24, 802.68it/s] 10%|█         | 7940/75722 [00:09<01:23, 815.55it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors
 11%|█         | 8028/75722 [00:09<01:21, 832.06it/s] 11%|█         | 8120/75722 [00:09<01:19, 855.38it/s] 11%|█         | 8208/75722 [00:09<01:19, 854.38it/s] 11%|█         | 8310/75722 [00:10<01:14, 900.69it/s] 11%|█         | 8404/75722 [00:10<01:13, 912.01it/s] 11%|█         | 8496/75722 [00:10<01:16, 881.26it/s] 11%|█▏        | 8585/75722 [00:10<01:16, 873.34it/s] 11%|█▏        | 8673/75722 [00:10<01:22, 813.66it/s] 12%|█▏        | 8757/75722 [00:10<01:21, 818.45it/s] 12%|█▏        | 8846/75722 [00:10<01:19, 838.68it/s] 12%|█▏        | 8931/75722 [00:10<01:19, 840.39it/s] 12%|█▏        | 9016/75722 [00:10<01:22, 808.40it/s] 12%|█▏        | 9098/75722 [00:10<01:22, 804.41it/s] 12%|█▏        | 9187/75722 [00:11<01:20, 827.78it/s] 12%|█▏        | 9271/75722 [00:11<01:24, 789.88it/s] 12%|█▏        | 9365/75722 [00:11<01:19, 830.60it/s] 13%|█▎        | 9478/75722 [00:11<01:12, 913.83it/s] 13%|█▎        | 9598/75722 [00:11<01:06, 996.94it/s] 13%|█▎        | 9706/75722 [00:11<01:04, 1018.24it/s] 13%|█▎        | 9809/75722 [00:11<01:06, 992.74it/s]  13%|█▎        | 9909/75722 [00:11<01:13, 900.43it/s] 13%|█▎        | 10001/75722 [00:11<01:19, 831.34it/s] 13%|█▎        | 10087/75722 [00:12<01:18, 838.11it/s] 13%|█▎        | 10173/75722 [00:12<01:17, 843.11it/s] 14%|█▎        | 10259/75722 [00:12<01:17, 842.12it/s] 14%|█▎        | 10344/75722 [00:12<01:19, 817.41it/s] 14%|█▍        | 10434/75722 [00:12<01:17, 839.10it/s] 14%|█▍        | 10520/75722 [00:12<01:17, 843.85it/s] 14%|█▍        | 10620/75722 [00:12<01:13, 885.83it/s] 14%|█▍        | 10719/75722 [00:12<01:11, 914.96it/s] 14%|█▍        | 10811/75722 [00:12<01:18, 822.65it/s] 14%|█▍        | 10902/75722 [00:13<01:16, 845.17it/s] 15%|█▍        | 10994/75722 [00:13<01:14, 866.03it/s] 15%|█▍        | 11099/75722 [00:13<01:10, 917.99it/s] 15%|█▍        | 11192/75722 [00:13<01:14, 864.28it/s] 15%|█▍        | 11285/75722 [00:13<01:13, 880.85it/s] 15%|█▌        | 11375/75722 [00:13<01:13, 875.40it/s] 15%|█▌        | 11464/75722 [00:13<01:25, 753.17it/s] 15%|█▌        | 11543/75722 [00:13<01:32, 695.73it/s] 15%|█▌        | 11616/75722 [00:13<01:38, 653.67it/s] 15%|█▌        | 11688/75722 [00:14<01:35, 668.74it/s] 16%|█▌        | 11757/75722 [00:14<01:35, 666.77it/s] 16%|█▌        | 11825/75722 [00:14<01:35, 669.80it/s] 16%|█▌        | 11912/75722 [00:14<01:28, 724.89it/s] 16%|█▌        | 12000/75722 [00:14<01:23, 767.47it/s] 16%|█▌        | 12090/75722 [00:14<01:19, 805.27it/s] 16%|█▌        | 12186/75722 [00:14<01:14, 849.75it/s] 16%|█▌        | 12281/75722 [00:14<01:12, 877.12it/s] 16%|█▋        | 12370/75722 [00:14<01:14, 854.32it/s] 16%|█▋        | 12456/75722 [00:15<01:18, 808.64it/s] 17%|█▋        | 12551/75722 [00:15<01:14, 847.21it/s] 17%|█▋        | 12661/75722 [00:15<01:08, 916.57it/s] 17%|█▋        | 12754/75722 [00:15<01:12, 873.14it/s] 17%|█▋        | 12850/75722 [00:15<01:10, 895.98it/s] 17%|█▋        | 12941/75722 [00:15<01:11, 881.73it/s] 17%|█▋        | 13030/75722 [00:15<01:11, 879.84it/s] 17%|█▋        | 13119/75722 [00:15<01:12, 859.33it/s] 17%|█▋        | 13214/75722 [00:15<01:10, 885.01it/s] 18%|█▊        | 13303/75722 [00:15<01:14, 841.40it/s] 18%|█▊        | 13396/75722 [00:16<01:12, 862.90it/s] 18%|█▊        | 13492/75722 [00:16<01:10, 887.76it/s] 18%|█▊        | 13582/75722 [00:16<01:13, 848.94it/s] 18%|█▊        | 13669/75722 [00:16<01:12, 853.94it/s] 18%|█▊        | 13759/75722 [00:16<01:11, 865.64it/s] 18%|█▊        | 13848/75722 [00:16<01:11, 870.63it/s] 18%|█▊        | 13936/75722 [00:16<01:13, 845.77it/s] 19%|█▊        | 14021/75722 [00:16<01:15, 819.82it/s] 19%|█▊        | 14104/75722 [00:16<01:20, 765.60it/s] 19%|█▊        | 14182/75722 [00:17<01:26, 714.93it/s] 19%|█▉        | 14255/75722 [00:17<01:29, 685.30it/s] 19%|█▉        | 14325/75722 [00:17<01:31, 668.72it/s] 19%|█▉        | 14393/75722 [00:17<01:32, 661.27it/s] 19%|█▉        | 14472/75722 [00:17<01:28, 695.53it/s] 19%|█▉        | 14543/75722 [00:17<01:28, 691.25it/s] 19%|█▉        | 14622/75722 [00:17<01:25, 717.23it/s] 19%|█▉        | 14695/75722 [00:17<01:25, 716.20it/s] 20%|█▉        | 14767/75722 [00:17<01:35, 636.82it/s] 20%|█▉        | 14833/75722 [00:18<01:35, 636.28it/s] 20%|█▉        | 14905/75722 [00:18<01:32, 656.88it/s] 20%|█▉        | 14972/75722 [00:18<01:33, 652.07it/s] 20%|█▉        | 15038/75722 [00:18<01:34, 644.56it/s] 20%|█▉        | 15111/75722 [00:18<01:30, 667.68it/s] 20%|██        | 15179/75722 [00:18<01:31, 662.63it/s] 20%|██        | 15254/75722 [00:18<01:28, 686.60it/s] 20%|██        | 15328/75722 [00:18<01:26, 700.51it/s] 20%|██        | 15399/75722 [00:18<01:27, 686.74it/s] 20%|██        | 15473/75722 [00:18<01:25, 701.03it/s] 21%|██        | 15544/75722 [00:19<01:25, 703.46it/s] 21%|██        | 15615/75722 [00:19<01:26, 695.54it/s] 21%|██        | 15687/75722 [00:19<01:25, 701.39it/s] 21%|██        | 15768/75722 [00:19<01:21, 733.22it/s] 21%|██        | 15842/75722 [00:19<01:24, 712.74it/s] 21%|██        | 15914/75722 [00:19<01:24, 706.91it/s] 21%|██        | 15991/75722 [00:19<01:22, 725.27it/s] 21%|██        | 16064/75722 [00:19<01:22, 722.18it/s] 21%|██▏       | 16137/75722 [00:19<01:23, 712.40it/s] 21%|██▏       | 16209/75722 [00:20<01:23, 708.95it/s] 21%|██▏       | 16280/75722 [00:20<01:24, 701.33it/s] 22%|██▏       | 16351/75722 [00:20<01:27, 679.55it/s] 22%|██▏       | 16420/75722 [00:20<01:30, 653.42it/s] 22%|██▏       | 16486/75722 [00:20<01:30, 654.82it/s] 22%|██▏       | 16553/75722 [00:20<01:30, 656.24it/s] 22%|██▏       | 16630/75722 [00:20<01:25, 688.22it/s] 22%|██▏       | 16703/75722 [00:20<01:24, 698.94it/s] 22%|██▏       | 16777/75722 [00:20<01:23, 710.07it/s] 22%|██▏       | 16849/75722 [00:20<01:22, 712.57it/s] 22%|██▏       | 16925/75722 [00:21<01:21, 725.24it/s] 22%|██▏       | 16998/75722 [00:21<01:22, 715.94it/s] 23%|██▎       | 17070/75722 [00:21<01:24, 697.57it/s] 23%|██▎       | 17143/75722 [00:21<01:23, 704.80it/s] 23%|██▎       | 17215/75722 [00:21<01:22, 706.68it/s] 23%|██▎       | 17286/75722 [00:21<01:23, 703.45it/s] 23%|██▎       | 17358/75722 [00:21<01:22, 706.74it/s] 23%|██▎       | 17436/75722 [00:21<01:20, 727.17it/s] 23%|██▎       | 17517/75722 [00:21<01:17, 750.54it/s] 23%|██▎       | 17593/75722 [00:21<01:17, 745.71it/s] 23%|██▎       | 17668/75722 [00:22<01:23, 694.76it/s] 23%|██▎       | 17739/75722 [00:22<01:23, 690.89it/s] 24%|██▎       | 17809/75722 [00:22<01:24, 683.39it/s] 24%|██▎       | 17880/75722 [00:22<01:24, 688.05it/s] 24%|██▎       | 17950/75722 [00:22<01:27, 660.43it/s] 24%|██▍       | 18017/75722 [00:22<01:32, 624.58it/s] 24%|██▍       | 18081/75722 [00:22<01:31, 627.86it/s] 24%|██▍       | 18154/75722 [00:22<01:27, 656.32it/s] 24%|██▍       | 18228/75722 [00:22<01:24, 679.88it/s] 24%|██▍       | 18299/75722 [00:23<01:23, 687.59it/s] 24%|██▍       | 18372/75722 [00:23<01:22, 699.39it/s] 24%|██▍       | 18443/75722 [00:23<01:24, 679.64it/s] 24%|██▍       | 18512/75722 [00:23<01:24, 678.15it/s] 25%|██▍       | 18583/75722 [00:23<01:23, 686.77it/s] 25%|██▍       | 18652/75722 [00:23<01:25, 665.68it/s] 25%|██▍       | 18726/75722 [00:23<01:23, 686.07it/s] 25%|██▍       | 18795/75722 [00:23<01:24, 676.28it/s] 25%|██▍       | 18863/75722 [00:23<01:33, 606.43it/s] 25%|██▍       | 18926/75722 [00:24<01:37, 581.50it/s] 25%|██▌       | 18987/75722 [00:24<01:36, 587.22it/s] 25%|██▌       | 19056/75722 [00:24<01:32, 613.49it/s] 25%|██▌       | 19128/75722 [00:24<01:28, 640.73it/s] 25%|██▌       | 19198/75722 [00:24<01:25, 657.61it/s] 25%|██▌       | 19273/75722 [00:24<01:22, 681.99it/s] 26%|██▌       | 19346/75722 [00:24<01:21, 693.72it/s] 26%|██▌       | 19423/75722 [00:24<01:18, 714.90it/s] 26%|██▌       | 19495/75722 [00:24<01:18, 711.88it/s] 26%|██▌       | 19568/75722 [00:24<01:18, 715.88it/s] 26%|██▌       | 19640/75722 [00:25<01:21, 690.91it/s] 26%|██▌       | 19710/75722 [00:25<01:21, 691.25it/s] 26%|██▌       | 19780/75722 [00:25<01:23, 673.69it/s] 26%|██▌       | 19848/75722 [00:25<01:26, 644.91it/s] 26%|██▋       | 19913/75722 [00:25<01:28, 632.69it/s] 26%|██▋       | 19977/75722 [00:25<01:30, 617.97it/s] 26%|██▋       | 20039/75722 [00:25<01:32, 601.33it/s] 27%|██▋       | 20100/75722 [00:25<01:32, 601.02it/s] 27%|██▋       | 20170/75722 [00:25<01:28, 628.69it/s] 27%|██▋       | 20234/75722 [00:26<01:31, 607.03it/s] 27%|██▋       | 20295/75722 [00:26<01:32, 601.87it/s] 27%|██▋       | 20378/75722 [00:26<01:22, 667.18it/s] 27%|██▋       | 20452/75722 [00:26<01:20, 685.61it/s] 27%|██▋       | 20526/75722 [00:26<01:18, 700.47it/s] 27%|██▋       | 20598/75722 [00:26<01:18, 704.31it/s] 27%|██▋       | 20675/75722 [00:26<01:16, 723.53it/s] 27%|██▋       | 20750/75722 [00:26<01:15, 729.59it/s] 28%|██▊       | 20824/75722 [00:26<01:17, 704.24it/s] 28%|██▊       | 20895/75722 [00:26<01:22, 661.15it/s] 28%|██▊       | 20962/75722 [00:27<01:24, 648.97it/s] 28%|██▊       | 21038/75722 [00:27<01:20, 677.56it/s] 28%|██▊       | 21112/75722 [00:27<01:18, 692.44it/s] 28%|██▊       | 21188/75722 [00:27<01:16, 711.12it/s] 28%|██▊       | 21260/75722 [00:27<01:16, 708.18it/s] 28%|██▊       | 21332/75722 [00:27<01:18, 696.69it/s] 28%|██▊       | 21402/75722 [00:27<01:20, 671.94it/s] 28%|██▊       | 21474/75722 [00:27<01:19, 685.46it/s] 28%|██▊       | 21550/75722 [00:27<01:16, 706.75it/s] 29%|██▊       | 21623/75722 [00:28<01:15, 713.15it/s] 29%|██▊       | 21700/75722 [00:28<01:14, 729.64it/s] 29%|██▉       | 21774/75722 [00:28<01:15, 716.66it/s] 29%|██▉       | 21846/75722 [00:28<01:15, 710.68it/s] 29%|██▉       | 21922/75722 [00:28<01:14, 724.46it/s] 29%|██▉       | 21996/75722 [00:28<01:13, 727.53it/s] 29%|██▉       | 22069/75722 [00:28<01:18, 682.92it/s] 29%|██▉       | 22138/75722 [00:29<03:26, 259.20it/s] 29%|██▉       | 22205/75722 [00:29<02:50, 313.21it/s] 29%|██▉       | 22273/75722 [00:29<02:23, 371.35it/s] 30%|██▉       | 22339/75722 [00:29<02:05, 424.59it/s] 30%|██▉       | 22411/75722 [00:29<01:49, 486.27it/s] 30%|██▉       | 22488/75722 [00:29<01:36, 550.40it/s] 30%|██▉       | 22565/75722 [00:29<01:28, 603.05it/s] 30%|██▉       | 22636/75722 [00:30<01:26, 611.10it/s] 30%|██▉       | 22713/75722 [00:30<01:21, 652.22it/s] 30%|███       | 22784/75722 [00:30<01:21, 652.18it/s] 30%|███       | 22854/75722 [00:30<01:20, 656.59it/s] 30%|███       | 22923/75722 [00:30<01:20, 654.10it/s] 30%|███       | 22991/75722 [00:30<01:20, 655.70it/s] 30%|███       | 23059/75722 [00:30<01:19, 660.63it/s] 31%|███       | 23127/75722 [00:30<01:20, 650.10it/s] 31%|███       | 23203/75722 [00:30<01:17, 680.81it/s] 31%|███       | 23272/75722 [00:31<01:19, 657.34it/s] 31%|███       | 23350/75722 [00:31<01:15, 690.26it/s] 31%|███       | 23422/75722 [00:31<01:14, 698.24it/s] 31%|███       | 23493/75722 [00:31<01:17, 676.78it/s] 31%|███       | 23562/75722 [00:31<01:18, 664.65it/s] 31%|███       | 23629/75722 [00:31<01:19, 658.15it/s] 31%|███▏      | 23708/75722 [00:31<01:14, 695.08it/s] 31%|███▏      | 23781/75722 [00:31<01:13, 703.91it/s] 31%|███▏      | 23852/75722 [00:31<01:16, 675.78it/s] 32%|███▏      | 23929/75722 [00:31<01:14, 699.73it/s] 32%|███▏      | 24000/75722 [00:32<01:14, 692.63it/s] 32%|███▏      | 24070/75722 [00:32<01:17, 665.46it/s] 32%|███▏      | 24142/75722 [00:32<01:15, 679.51it/s] 32%|███▏      | 24212/75722 [00:32<01:15, 683.91it/s] 32%|███▏      | 24281/75722 [00:32<01:15, 680.22it/s] 32%|███▏      | 24350/75722 [00:32<01:17, 660.29it/s] 32%|███▏      | 24427/75722 [00:32<01:14, 688.19it/s] 32%|███▏      | 24497/75722 [00:32<01:15, 682.69it/s] 32%|███▏      | 24566/75722 [00:32<01:15, 676.37it/s] 33%|███▎      | 24634/75722 [00:33<01:18, 654.01it/s] 33%|███▎      | 24700/75722 [00:33<01:20, 633.22it/s] 33%|███▎      | 24766/75722 [00:33<01:19, 640.08it/s] 33%|███▎      | 24843/75722 [00:33<01:15, 675.04it/s] 33%|███▎      | 24917/75722 [00:33<01:13, 692.71it/s] 33%|███▎      | 24990/75722 [00:33<01:12, 700.68it/s] 33%|███▎      | 25061/75722 [00:33<01:15, 675.33it/s] 33%|███▎      | 25135/75722 [00:33<01:13, 692.87it/s] 33%|███▎      | 25205/75722 [00:33<01:12, 692.06it/s] 33%|███▎      | 25275/75722 [00:33<01:14, 678.02it/s] 33%|███▎      | 25344/75722 [00:34<01:14, 680.74it/s] 34%|███▎      | 25413/75722 [00:34<01:13, 681.15it/s] 34%|███▎      | 25482/75722 [00:34<01:15, 669.85it/s] 34%|███▎      | 25550/75722 [00:34<01:17, 645.59it/s] 34%|███▍      | 25615/75722 [00:34<01:17, 643.11it/s] 34%|███▍      | 25680/75722 [00:34<01:20, 621.87it/s] 34%|███▍      | 25746/75722 [00:34<01:19, 630.55it/s] 34%|███▍      | 25810/75722 [00:34<01:19, 626.55it/s] 34%|███▍      | 25873/75722 [00:34<01:22, 607.35it/s] 34%|███▍      | 25956/75722 [00:34<01:14, 669.20it/s] 34%|███▍      | 26033/75722 [00:35<01:11, 698.19it/s] 34%|███▍      | 26112/75722 [00:35<01:08, 722.91it/s] 35%|███▍      | 26185/75722 [00:35<01:08, 721.95it/s] 35%|███▍      | 26258/75722 [00:35<01:09, 709.83it/s] 35%|███▍      | 26330/75722 [00:35<01:09, 706.92it/s] 35%|███▍      | 26401/75722 [00:35<01:11, 691.41it/s] 35%|███▍      | 26471/75722 [00:35<01:11, 692.47it/s] 35%|███▌      | 26544/75722 [00:35<01:09, 703.28it/s] 35%|███▌      | 26615/75722 [00:35<01:11, 688.68it/s] 35%|███▌      | 26684/75722 [00:36<01:11, 687.17it/s] 35%|███▌      | 26753/75722 [00:36<01:14, 661.17it/s] 35%|███▌      | 26820/75722 [00:36<01:15, 649.00it/s] 36%|███▌      | 26886/75722 [00:36<01:18, 622.79it/s] 36%|███▌      | 26951/75722 [00:36<01:17, 628.49it/s] 36%|███▌      | 27015/75722 [00:36<01:17, 631.77it/s] 36%|███▌      | 27079/75722 [00:36<01:17, 626.41it/s] 36%|███▌      | 27143/75722 [00:36<01:17, 629.95it/s] 36%|███▌      | 27207/75722 [00:36<01:19, 611.37it/s] 36%|███▌      | 27269/75722 [00:36<01:21, 591.22it/s] 36%|███▌      | 27346/75722 [00:37<01:15, 640.68it/s] 36%|███▌      | 27414/75722 [00:37<01:14, 648.26it/s] 36%|███▋      | 27494/75722 [00:37<01:09, 691.89it/s] 36%|███▋      | 27571/75722 [00:37<01:07, 712.04it/s] 37%|███▋      | 27643/75722 [00:37<01:10, 684.63it/s] 37%|███▋      | 27712/75722 [00:37<01:12, 657.74it/s] 37%|███▋      | 27779/75722 [00:37<01:14, 644.71it/s] 37%|███▋      | 27854/75722 [00:37<01:11, 672.33it/s] 37%|███▋      | 27929/75722 [00:37<01:09, 692.40it/s] 37%|███▋      | 28002/75722 [00:38<01:08, 700.51it/s] 37%|███▋      | 28073/75722 [00:38<01:10, 678.70it/s] 37%|███▋      | 28144/75722 [00:38<01:09, 683.62it/s] 37%|███▋      | 28213/75722 [00:38<01:11, 667.44it/s] 37%|███▋      | 28289/75722 [00:38<01:08, 692.04it/s] 37%|███▋      | 28359/75722 [00:38<01:08, 692.97it/s] 38%|███▊      | 28433/75722 [00:38<01:07, 704.23it/s] 38%|███▊      | 28510/75722 [00:38<01:05, 723.14it/s] 38%|███▊      | 28583/75722 [00:38<01:05, 723.77it/s] 38%|███▊      | 28662/75722 [00:38<01:03, 741.11it/s] 38%|███▊      | 28738/75722 [00:39<01:02, 746.68it/s] 38%|███▊      | 28813/75722 [00:39<01:04, 725.39it/s] 38%|███▊      | 28886/75722 [00:39<01:05, 717.90it/s] 38%|███▊      | 28958/75722 [00:39<01:07, 695.70it/s] 38%|███▊      | 29028/75722 [00:39<01:08, 679.47it/s] 38%|███▊      | 29100/75722 [00:39<01:07, 689.17it/s] 39%|███▊      | 29177/75722 [00:39<01:05, 712.25it/s] 39%|███▊      | 29255/75722 [00:39<01:03, 730.10it/s] 39%|███▊      | 29329/75722 [00:39<01:08, 681.34it/s] 39%|███▉      | 29398/75722 [00:40<01:11, 649.02it/s] 39%|███▉      | 29474/75722 [00:40<01:08, 679.02it/s] 39%|███▉      | 29545/75722 [00:40<01:07, 686.19it/s] 39%|███▉      | 29616/75722 [00:40<01:06, 692.89it/s] 39%|███▉      | 29699/75722 [00:40<01:02, 730.55it/s] 39%|███▉      | 29773/75722 [00:40<01:03, 728.41it/s] 39%|███▉      | 29847/75722 [00:40<01:05, 703.33it/s] 40%|███▉      | 29918/75722 [00:40<01:06, 689.63it/s] 40%|███▉      | 29992/75722 [00:40<01:04, 703.91it/s] 40%|███▉      | 30073/75722 [00:40<01:02, 733.50it/s] 40%|███▉      | 30147/75722 [00:41<01:02, 724.81it/s] 40%|███▉      | 30223/75722 [00:41<01:01, 734.83it/s] 40%|████      | 30297/75722 [00:41<01:01, 733.93it/s] 40%|████      | 30376/75722 [00:41<01:00, 748.72it/s] 40%|████      | 30451/75722 [00:41<01:00, 748.75it/s] 40%|████      | 30528/75722 [00:41<01:00, 752.72it/s] 40%|████      | 30606/75722 [00:41<00:59, 759.79it/s] 41%|████      | 30683/75722 [00:41<01:00, 740.50it/s] 41%|████      | 30758/75722 [00:41<01:01, 732.32it/s] 41%|████      | 30841/75722 [00:42<00:59, 757.24it/s] 41%|████      | 30920/75722 [00:42<00:58, 766.82it/s] 41%|████      | 30997/75722 [00:42<00:58, 759.69it/s] 41%|████      | 31074/75722 [00:42<00:59, 744.80it/s] 41%|████      | 31149/75722 [00:42<01:01, 728.10it/s] 41%|████      | 31223/75722 [00:42<01:00, 730.18it/s] 41%|████▏     | 31297/75722 [00:42<01:01, 722.94it/s] 41%|████▏     | 31375/75722 [00:42<01:00, 737.54it/s] 42%|████▏     | 31449/75722 [00:42<01:00, 735.74it/s] 42%|████▏     | 31533/75722 [00:42<00:57, 766.34it/s] 42%|████▏     | 31610/75722 [00:43<00:59, 744.78it/s] 42%|████▏     | 31685/75722 [00:43<01:01, 715.92it/s] 42%|████▏     | 31760/75722 [00:43<01:00, 725.27it/s] 42%|████▏     | 31833/75722 [00:43<01:01, 716.03it/s] 42%|████▏     | 31908/75722 [00:43<01:00, 724.04it/s] 42%|████▏     | 31990/75722 [00:43<00:58, 747.08it/s] 42%|████▏     | 32065/75722 [00:43<00:58, 741.65it/s] 42%|████▏     | 32140/75722 [00:43<00:59, 737.15it/s] 43%|████▎     | 32214/75722 [00:43<01:01, 706.37it/s] 43%|████▎     | 32289/75722 [00:43<01:00, 716.53it/s] 43%|████▎     | 32364/75722 [00:44<00:59, 726.16it/s] 43%|████▎     | 32437/75722 [00:44<01:01, 703.90it/s] 43%|████▎     | 32513/75722 [00:44<01:00, 717.26it/s] 43%|████▎     | 32588/75722 [00:44<00:59, 724.75it/s] 43%|████▎     | 32661/75722 [00:44<01:00, 713.82it/s] 43%|████▎     | 32733/75722 [00:44<01:02, 686.29it/s] 43%|████▎     | 32802/75722 [00:44<01:06, 647.08it/s] 43%|████▎     | 32868/75722 [00:44<01:06, 641.35it/s] 43%|████▎     | 32933/75722 [00:44<01:06, 638.80it/s] 44%|████▎     | 33006/75722 [00:45<01:04, 663.30it/s] 44%|████▎     | 33079/75722 [00:45<01:02, 681.40it/s] 44%|████▍     | 33153/75722 [00:45<01:01, 697.39it/s] 44%|████▍     | 33230/75722 [00:45<00:59, 718.15it/s] 44%|████▍     | 33303/75722 [00:45<00:59, 710.58it/s] 44%|████▍     | 33375/75722 [00:45<01:02, 676.23it/s] 44%|████▍     | 33444/75722 [00:45<01:02, 675.04it/s] 44%|████▍     | 33513/75722 [00:45<01:02, 679.03it/s] 44%|████▍     | 33584/75722 [00:45<01:01, 684.63it/s] 44%|████▍     | 33653/75722 [00:45<01:01, 679.75it/s] 45%|████▍     | 33722/75722 [00:46<01:02, 677.30it/s] 45%|████▍     | 33790/75722 [00:46<01:02, 675.69it/s] 45%|████▍     | 33858/75722 [00:46<01:06, 628.55it/s] 45%|████▍     | 33933/75722 [00:46<01:03, 661.75it/s] 45%|████▍     | 34001/75722 [00:46<01:02, 663.62it/s] 45%|████▌     | 34076/75722 [00:46<01:00, 684.32it/s] 45%|████▌     | 34145/75722 [00:46<01:03, 657.64it/s] 45%|████▌     | 34215/75722 [00:46<01:02, 667.23it/s] 45%|████▌     | 34284/75722 [00:46<01:01, 672.76it/s] 45%|████▌     | 34352/75722 [00:47<01:03, 656.47it/s] 45%|████▌     | 34419/75722 [00:47<01:02, 660.24it/s] 46%|████▌     | 34486/75722 [00:47<01:03, 649.88it/s] 46%|████▌     | 34552/75722 [00:47<01:03, 643.87it/s] 46%|████▌     | 34623/75722 [00:47<01:02, 661.54it/s] 46%|████▌     | 34690/75722 [00:47<01:02, 660.41it/s] 46%|████▌     | 34757/75722 [00:47<01:04, 637.98it/s] 46%|████▌     | 34823/75722 [00:47<01:03, 643.07it/s] 46%|████▌     | 34888/75722 [00:47<01:07, 608.35it/s] 46%|████▌     | 34950/75722 [00:48<01:08, 597.46it/s] 46%|████▋     | 35024/75722 [00:48<01:03, 636.75it/s] 46%|████▋     | 35101/75722 [00:48<01:00, 674.42it/s] 46%|████▋     | 35174/75722 [00:48<00:58, 688.80it/s] 47%|████▋     | 35257/75722 [00:48<00:55, 727.20it/s] 47%|████▋     | 35333/75722 [00:48<00:54, 735.64it/s] 47%|████▋     | 35409/75722 [00:48<00:54, 739.54it/s] 47%|████▋     | 35484/75722 [00:48<00:55, 729.11it/s] 47%|████▋     | 35562/75722 [00:48<00:54, 741.15it/s] 47%|████▋     | 35642/75722 [00:48<00:52, 756.81it/s] 47%|████▋     | 35718/75722 [00:49<00:53, 746.64it/s] 47%|████▋     | 35798/75722 [00:49<00:52, 761.92it/s] 47%|████▋     | 35877/75722 [00:49<00:51, 769.46it/s] 47%|████▋     | 35955/75722 [00:49<00:55, 719.81it/s] 48%|████▊     | 36028/75722 [00:49<00:55, 712.38it/s] 48%|████▊     | 36100/75722 [00:49<00:57, 685.21it/s] 48%|████▊     | 36172/75722 [00:49<00:56, 694.10it/s] 48%|████▊     | 36250/75722 [00:49<00:54, 717.87it/s] 48%|████▊     | 36323/75722 [00:49<00:55, 704.54it/s] 48%|████▊     | 36396/75722 [00:49<00:55, 711.17it/s] 48%|████▊     | 36468/75722 [00:50<00:55, 704.24it/s] 48%|████▊     | 36542/75722 [00:50<00:55, 711.13it/s] 48%|████▊     | 36615/75722 [00:50<00:54, 714.58it/s] 48%|████▊     | 36687/75722 [00:50<00:54, 715.62it/s] 49%|████▊     | 36759/75722 [00:50<00:55, 702.43it/s] 49%|████▊     | 36835/75722 [00:50<00:54, 716.98it/s] 49%|████▊     | 36907/75722 [00:50<00:56, 687.94it/s] 49%|████▉     | 36977/75722 [00:50<00:57, 679.17it/s] 49%|████▉     | 37046/75722 [00:50<00:58, 661.11it/s] 49%|████▉     | 37113/75722 [00:51<00:59, 652.83it/s] 49%|████▉     | 37180/75722 [00:51<00:58, 654.49it/s] 49%|████▉     | 37247/75722 [00:51<00:58, 657.01it/s] 49%|████▉     | 37326/75722 [00:51<00:55, 694.09it/s] 49%|████▉     | 37408/75722 [00:51<00:52, 730.85it/s] 49%|████▉     | 37482/75722 [00:51<00:55, 693.94it/s] 50%|████▉     | 37559/75722 [00:51<00:53, 714.16it/s] 50%|████▉     | 37637/75722 [00:51<00:51, 732.68it/s] 50%|████▉     | 37711/75722 [00:51<00:52, 718.89it/s] 50%|████▉     | 37784/75722 [00:51<00:55, 688.47it/s] 50%|████▉     | 37854/75722 [00:52<00:56, 666.74it/s] 50%|█████     | 37922/75722 [00:52<00:57, 660.69it/s] 50%|█████     | 37989/75722 [00:52<00:58, 641.30it/s] 50%|█████     | 38061/75722 [00:52<00:56, 662.63it/s] 50%|█████     | 38140/75722 [00:52<00:53, 697.52it/s] 50%|█████     | 38214/75722 [00:52<00:52, 708.72it/s] 51%|█████     | 38286/75722 [00:52<00:55, 671.27it/s] 51%|█████     | 38354/75722 [00:52<00:56, 663.61it/s] 51%|█████     | 38438/75722 [00:52<00:52, 712.77it/s] 51%|█████     | 38510/75722 [00:53<00:53, 696.72it/s] 51%|█████     | 38587/75722 [00:53<00:51, 715.41it/s] 51%|█████     | 38659/75722 [00:53<00:53, 691.02it/s] 51%|█████     | 38729/75722 [00:53<00:53, 688.69it/s] 51%|█████     | 38799/75722 [00:53<00:54, 678.85it/s] 51%|█████▏    | 38870/75722 [00:53<00:53, 686.53it/s] 51%|█████▏    | 38939/75722 [00:53<00:53, 684.83it/s] 52%|█████▏    | 39013/75722 [00:53<00:52, 697.99it/s] 52%|█████▏    | 39084/75722 [00:53<00:52, 700.24it/s] 52%|█████▏    | 39158/75722 [00:53<00:51, 710.35it/s] 52%|█████▏    | 39238/75722 [00:54<00:49, 735.72it/s] 52%|█████▏    | 39312/75722 [00:54<00:49, 735.37it/s] 52%|█████▏    | 39387/75722 [00:54<00:49, 738.19it/s] 52%|█████▏    | 39470/75722 [00:54<00:47, 763.02it/s] 52%|█████▏    | 39547/75722 [00:54<00:48, 751.41it/s] 52%|█████▏    | 39623/75722 [00:54<00:48, 740.03it/s] 52%|█████▏    | 39698/75722 [00:54<00:48, 741.75it/s] 53%|█████▎    | 39781/75722 [00:54<00:46, 765.92it/s] 53%|█████▎    | 39858/75722 [00:54<00:50, 716.40it/s] 53%|█████▎    | 39931/75722 [00:55<00:54, 661.02it/s] 53%|█████▎    | 40003/75722 [00:55<00:52, 676.15it/s] 53%|█████▎    | 40072/75722 [00:55<00:52, 679.81it/s] 53%|█████▎    | 40147/75722 [00:55<00:50, 697.60it/s] 53%|█████▎    | 40219/75722 [00:55<00:50, 703.74it/s] 53%|█████▎    | 40290/75722 [00:55<00:50, 699.13it/s] 53%|█████▎    | 40361/75722 [01:01<14:35, 40.40it/s]  53%|█████▎    | 40417/75722 [01:01<11:11, 52.58it/s] 53%|█████▎    | 40485/75722 [01:01<08:03, 72.89it/s] 54%|█████▎    | 40548/75722 [01:01<06:00, 97.49it/s] 54%|█████▎    | 40608/75722 [01:01<04:35, 127.27it/s] 54%|█████▎    | 40676/75722 [01:01<03:25, 170.46it/s] 54%|█████▍    | 40749/75722 [01:01<02:34, 226.42it/s] 54%|█████▍    | 40823/75722 [01:01<02:00, 290.52it/s] 54%|█████▍    | 40891/75722 [01:02<01:41, 344.76it/s] 54%|█████▍    | 40972/75722 [01:02<01:21, 425.94it/s] 54%|█████▍    | 41045/75722 [01:02<01:11, 485.17it/s] 54%|█████▍    | 41128/75722 [01:02<01:01, 561.70it/s] 54%|█████▍    | 41213/75722 [01:02<00:54, 629.79it/s] 55%|█████▍    | 41291/75722 [01:02<00:52, 653.83it/s] 55%|█████▍    | 41369/75722 [01:02<00:50, 686.20it/s] 55%|█████▍    | 41446/75722 [01:02<00:48, 700.92it/s] 55%|█████▍    | 41522/75722 [01:02<00:48, 699.66it/s] 55%|█████▍    | 41596/75722 [01:03<00:49, 685.97it/s] 55%|█████▌    | 41668/75722 [01:03<00:49, 689.86it/s] 55%|█████▌    | 41739/75722 [01:03<00:50, 670.31it/s] 55%|█████▌    | 41814/75722 [01:03<00:49, 690.44it/s] 55%|█████▌    | 41885/75722 [01:03<00:49, 680.68it/s] 55%|█████▌    | 41954/75722 [01:03<00:49, 683.13it/s] 55%|█████▌    | 42023/75722 [01:03<00:49, 682.75it/s] 56%|█████▌    | 42092/75722 [01:03<00:49, 679.55it/s] 56%|█████▌    | 42161/75722 [01:03<00:50, 665.41it/s] 56%|█████▌    | 42233/75722 [01:03<00:49, 679.98it/s] 56%|█████▌    | 42310/75722 [01:04<00:47, 705.65it/s] 56%|█████▌    | 42390/75722 [01:04<00:45, 728.65it/s] 56%|█████▌    | 42464/75722 [01:04<00:49, 668.99it/s] 56%|█████▌    | 42539/75722 [01:04<00:48, 690.13it/s] 56%|█████▋    | 42622/75722 [01:04<00:45, 728.87it/s] 56%|█████▋    | 42696/75722 [01:04<00:45, 721.68it/s] 56%|█████▋    | 42769/75722 [01:04<00:45, 723.72it/s] 57%|█████▋    | 42843/75722 [01:04<00:45, 727.17it/s] 57%|█████▋    | 42917/75722 [01:04<00:46, 708.25it/s] 57%|█████▋    | 42990/75722 [01:05<00:45, 714.48it/s] 57%|█████▋    | 43062/75722 [01:05<00:46, 704.52it/s] 57%|█████▋    | 43133/75722 [01:05<00:48, 672.91it/s] 57%|█████▋    | 43201/75722 [01:05<00:49, 656.42it/s] 57%|█████▋    | 43267/75722 [01:05<00:52, 622.41it/s] 57%|█████▋    | 43349/75722 [01:05<00:47, 675.94it/s] 57%|█████▋    | 43422/75722 [01:05<00:46, 687.84it/s] 57%|█████▋    | 43492/75722 [01:05<00:49, 652.43it/s] 58%|█████▊    | 43566/75722 [01:05<00:47, 676.80it/s] 58%|█████▊    | 43645/75722 [01:05<00:45, 707.62it/s] 58%|█████▊    | 43718/75722 [01:06<00:44, 711.61it/s] 58%|█████▊    | 43793/75722 [01:06<00:44, 721.17it/s] 58%|█████▊    | 43871/75722 [01:06<00:43, 737.27it/s] 58%|█████▊    | 43945/75722 [01:06<00:43, 725.04it/s] 58%|█████▊    | 44018/75722 [01:06<00:44, 714.72it/s] 58%|█████▊    | 44091/75722 [01:06<00:44, 717.45it/s] 58%|█████▊    | 44169/75722 [01:06<00:42, 734.33it/s] 58%|█████▊    | 44243/75722 [01:06<00:43, 724.41it/s] 59%|█████▊    | 44316/75722 [01:06<00:43, 718.31it/s] 59%|█████▊    | 44388/75722 [01:07<00:45, 681.25it/s] 59%|█████▊    | 44457/75722 [01:07<00:47, 664.28it/s] 59%|█████▉    | 44524/75722 [01:07<00:48, 641.03it/s] 59%|█████▉    | 44592/75722 [01:07<00:47, 650.57it/s] 59%|█████▉    | 44659/75722 [01:07<00:47, 654.54it/s] 59%|█████▉    | 44740/75722 [01:07<00:44, 699.41it/s] 59%|█████▉    | 44816/75722 [01:07<00:43, 715.85it/s] 59%|█████▉    | 44889/75722 [01:07<00:42, 719.10it/s] 59%|█████▉    | 44971/75722 [01:07<00:41, 747.90it/s] 59%|█████▉    | 45047/75722 [01:07<00:40, 750.50it/s] 60%|█████▉    | 45123/75722 [01:08<00:42, 725.33it/s] 60%|█████▉    | 45201/75722 [01:08<00:41, 739.36it/s] 60%|█████▉    | 45276/75722 [01:08<00:41, 730.91it/s] 60%|█████▉    | 45352/75722 [01:08<00:41, 739.25it/s] 60%|█████▉    | 45427/75722 [01:08<00:41, 737.85it/s] 60%|██████    | 45501/75722 [01:08<00:42, 713.07it/s] 60%|██████    | 45573/75722 [01:08<00:43, 686.39it/s] 60%|██████    | 45649/75722 [01:08<00:42, 704.25it/s] 60%|██████    | 45720/75722 [01:08<00:43, 694.76it/s] 60%|██████    | 45798/75722 [01:09<00:41, 717.29it/s] 61%|██████    | 45876/75722 [01:09<00:40, 734.96it/s] 61%|██████    | 45951/75722 [01:09<00:40, 738.73it/s] 61%|██████    | 46027/75722 [01:09<00:39, 742.94it/s] 61%|██████    | 46102/75722 [01:09<00:40, 739.06it/s] 61%|██████    | 46176/75722 [01:09<00:41, 708.61it/s] 61%|██████    | 46248/75722 [01:09<00:43, 683.01it/s] 61%|██████    | 46322/75722 [01:09<00:42, 697.60it/s] 61%|██████▏   | 46393/75722 [01:09<00:42, 697.26it/s] 61%|██████▏   | 46469/75722 [01:09<00:41, 712.49it/s] 61%|██████▏   | 46544/75722 [01:10<00:40, 721.92it/s] 62%|██████▏   | 46620/75722 [01:10<00:39, 730.77it/s] 62%|██████▏   | 46694/75722 [01:10<00:40, 719.04it/s] 62%|██████▏   | 46770/75722 [01:10<00:39, 729.94it/s] 62%|██████▏   | 46848/75722 [01:10<00:38, 744.05it/s] 62%|██████▏   | 46930/75722 [01:10<00:37, 764.79it/s] 62%|██████▏   | 47007/75722 [01:10<00:39, 728.68it/s] 62%|██████▏   | 47081/75722 [01:10<00:41, 697.59it/s] 62%|██████▏   | 47156/75722 [01:10<00:40, 711.42it/s] 62%|██████▏   | 47229/75722 [01:10<00:39, 714.56it/s] 62%|██████▏   | 47310/75722 [01:11<00:38, 741.35it/s] 63%|██████▎   | 47389/75722 [01:11<00:37, 753.40it/s] 63%|██████▎   | 47467/75722 [01:11<00:37, 759.89it/s] 63%|██████▎   | 47544/75722 [01:11<00:37, 755.88it/s] 63%|██████▎   | 47624/75722 [01:11<00:36, 767.30it/s] 63%|██████▎   | 47701/75722 [01:11<00:36, 761.74it/s] 63%|██████▎   | 47778/75722 [01:11<00:37, 754.39it/s] 63%|██████▎   | 47860/75722 [01:11<00:36, 772.59it/s] 63%|██████▎   | 47938/75722 [01:11<00:35, 773.07it/s] 63%|██████▎   | 48022/75722 [01:12<00:34, 791.68it/s] 64%|██████▎   | 48102/75722 [01:12<00:35, 773.29it/s] 64%|██████▎   | 48180/75722 [01:12<00:37, 734.29it/s] 64%|██████▎   | 48260/75722 [01:12<00:36, 751.34it/s] 64%|██████▍   | 48342/75722 [01:12<00:35, 770.48it/s] 64%|██████▍   | 48421/75722 [01:12<00:35, 775.91it/s] 64%|██████▍   | 48501/75722 [01:12<00:34, 781.97it/s] 64%|██████▍   | 48580/75722 [01:12<00:35, 759.01it/s] 64%|██████▍   | 48657/75722 [01:12<00:37, 728.58it/s] 64%|██████▍   | 48731/75722 [01:12<00:38, 698.60it/s] 64%|██████▍   | 48813/75722 [01:13<00:36, 730.18it/s] 65%|██████▍   | 48895/75722 [01:13<00:35, 755.03it/s] 65%|██████▍   | 48975/75722 [01:13<00:34, 767.01it/s] 65%|██████▍   | 49055/75722 [01:13<00:34, 775.04it/s] 65%|██████▍   | 49133/75722 [01:13<00:34, 761.45it/s] 65%|██████▍   | 49210/75722 [01:13<00:38, 694.12it/s] 65%|██████▌   | 49281/75722 [01:13<00:38, 687.83it/s] 65%|██████▌   | 49351/75722 [01:13<00:40, 655.10it/s] 65%|██████▌   | 49428/75722 [01:13<00:38, 685.28it/s] 65%|██████▌   | 49501/75722 [01:14<00:37, 696.75it/s] 65%|██████▌   | 49572/75722 [01:14<00:37, 695.12it/s] 66%|██████▌   | 49645/75722 [01:14<00:37, 702.28it/s] 66%|██████▌   | 49720/75722 [01:14<00:36, 713.06it/s] 66%|██████▌   | 49794/75722 [01:14<00:36, 720.01it/s] 66%|██████▌   | 49867/75722 [01:14<00:35, 722.45it/s] 66%|██████▌   | 49946/75722 [01:14<00:34, 740.41it/s] 66%|██████▌   | 50021/75722 [01:14<00:35, 723.08it/s] 66%|██████▌   | 50100/75722 [01:14<00:34, 739.72it/s] 66%|██████▋   | 50181/75722 [01:14<00:33, 758.44it/s] 66%|██████▋   | 50257/75722 [01:15<00:34, 739.01it/s] 66%|██████▋   | 50339/75722 [01:15<00:33, 759.94it/s] 67%|██████▋   | 50416/75722 [01:15<00:35, 721.50it/s] 67%|██████▋   | 50489/75722 [01:15<00:34, 723.78it/s] 67%|██████▋   | 50562/75722 [01:15<00:36, 692.30it/s] 67%|██████▋   | 50636/75722 [01:15<00:35, 703.04it/s] 67%|██████▋   | 50707/75722 [01:15<00:36, 684.67it/s] 67%|██████▋   | 50781/75722 [01:15<00:35, 698.85it/s] 67%|██████▋   | 50855/75722 [01:15<00:35, 709.45it/s] 67%|██████▋   | 50939/75722 [01:16<00:33, 745.72it/s] 67%|██████▋   | 51018/75722 [01:16<00:32, 756.63it/s] 67%|██████▋   | 51094/75722 [01:16<00:32, 747.24it/s] 68%|██████▊   | 51169/75722 [01:16<00:34, 715.55it/s] 68%|██████▊   | 51241/75722 [01:16<00:34, 714.14it/s] 68%|██████▊   | 51313/75722 [01:16<00:35, 689.41it/s] 68%|██████▊   | 51383/75722 [01:16<00:35, 683.12it/s] 68%|██████▊   | 51453/75722 [01:16<00:35, 685.09it/s] 68%|██████▊   | 51531/75722 [01:16<00:33, 711.75it/s] 68%|██████▊   | 51603/75722 [01:16<00:33, 709.68it/s] 68%|██████▊   | 51675/75722 [01:17<00:34, 696.92it/s] 68%|██████▊   | 51754/75722 [01:17<00:33, 721.47it/s] 68%|██████▊   | 51836/75722 [01:17<00:31, 750.15it/s] 69%|██████▊   | 51913/75722 [01:17<00:31, 753.31it/s] 69%|██████▊   | 51992/75722 [01:17<00:31, 763.39it/s] 69%|██████▉   | 52081/75722 [01:17<00:29, 797.88it/s] 69%|██████▉   | 52161/75722 [01:17<00:30, 765.97it/s] 69%|██████▉   | 52238/75722 [01:17<00:31, 752.23it/s] 69%|██████▉   | 52318/75722 [01:17<00:30, 763.91it/s] 69%|██████▉   | 52395/75722 [01:18<00:30, 758.05it/s] 69%|██████▉   | 52471/75722 [01:18<00:31, 730.94it/s] 69%|██████▉   | 52545/75722 [01:18<00:32, 714.84it/s] 69%|██████▉   | 52617/75722 [01:18<00:32, 714.39it/s] 70%|██████▉   | 52692/75722 [01:18<00:31, 721.35it/s] 70%|██████▉   | 52765/75722 [01:18<00:33, 681.74it/s] 70%|██████▉   | 52834/75722 [01:18<00:33, 680.74it/s] 70%|██████▉   | 52906/75722 [01:18<00:33, 690.76it/s] 70%|██████▉   | 52976/75722 [01:18<00:34, 665.83it/s] 70%|███████   | 53044/75722 [01:18<00:33, 669.32it/s] 70%|███████   | 53112/75722 [01:19<00:33, 669.39it/s] 70%|███████   | 53188/75722 [01:19<00:32, 693.36it/s] 70%|███████   | 53258/75722 [01:19<00:34, 656.14it/s] 70%|███████   | 53325/75722 [01:19<00:34, 641.38it/s] 71%|███████   | 53390/75722 [01:19<00:34, 641.75it/s] 71%|███████   | 53466/75722 [01:19<00:33, 674.36it/s] 71%|███████   | 53543/75722 [01:19<00:31, 701.95it/s] 71%|███████   | 53617/75722 [01:19<00:31, 709.47it/s] 71%|███████   | 53689/75722 [01:19<00:31, 691.77it/s] 71%|███████   | 53759/75722 [01:20<00:32, 680.53it/s] 71%|███████   | 53836/75722 [01:20<00:31, 704.68it/s] 71%|███████   | 53911/75722 [01:20<00:30, 715.63it/s] 71%|███████▏  | 53986/75722 [01:20<00:30, 723.82it/s] 71%|███████▏  | 54059/75722 [01:20<00:30, 706.93it/s] 71%|███████▏  | 54131/75722 [01:20<00:30, 709.35it/s] 72%|███████▏  | 54210/75722 [01:20<00:29, 731.09it/s] 72%|███████▏  | 54284/75722 [01:20<00:29, 727.79it/s] 72%|███████▏  | 54357/75722 [01:20<00:29, 713.49it/s] 72%|███████▏  | 54435/75722 [01:20<00:29, 731.01it/s] 72%|███████▏  | 54509/75722 [01:21<00:28, 732.65it/s] 72%|███████▏  | 54583/75722 [01:21<00:29, 711.00it/s] 72%|███████▏  | 54656/75722 [01:21<00:29, 713.49it/s] 72%|███████▏  | 54728/75722 [01:21<00:30, 697.51it/s] 72%|███████▏  | 54798/75722 [01:21<00:30, 684.79it/s] 72%|███████▏  | 54876/75722 [01:21<00:29, 712.08it/s] 73%|███████▎  | 54953/75722 [01:21<00:28, 727.27it/s] 73%|███████▎  | 55026/75722 [01:21<00:28, 721.27it/s] 73%|███████▎  | 55104/75722 [01:21<00:27, 738.35it/s] 73%|███████▎  | 55179/75722 [01:21<00:27, 740.47it/s] 73%|███████▎  | 55254/75722 [01:22<00:27, 740.07it/s] 73%|███████▎  | 55330/75722 [01:22<00:27, 743.77it/s] 73%|███████▎  | 55405/75722 [01:22<00:27, 735.07it/s] 73%|███████▎  | 55479/75722 [01:22<00:27, 735.19it/s] 73%|███████▎  | 55553/75722 [01:22<00:28, 715.64it/s] 73%|███████▎  | 55626/75722 [01:22<00:27, 718.98it/s] 74%|███████▎  | 55706/75722 [01:22<00:27, 740.81it/s] 74%|███████▎  | 55781/75722 [01:22<00:28, 703.80it/s] 74%|███████▍  | 55852/75722 [01:22<00:29, 677.01it/s] 74%|███████▍  | 55933/75722 [01:23<00:27, 713.93it/s] 74%|███████▍  | 56007/75722 [01:23<00:27, 719.52it/s] 74%|███████▍  | 56087/75722 [01:23<00:26, 742.13it/s] 74%|███████▍  | 56162/75722 [01:23<00:26, 730.41it/s] 74%|███████▍  | 56241/75722 [01:23<00:26, 745.20it/s] 74%|███████▍  | 56316/75722 [01:23<00:26, 742.08it/s] 74%|███████▍  | 56391/75722 [01:23<00:26, 736.17it/s] 75%|███████▍  | 56466/75722 [01:23<00:26, 737.89it/s] 75%|███████▍  | 56540/75722 [01:23<00:26, 726.26it/s] 75%|███████▍  | 56613/75722 [01:23<00:28, 678.28it/s] 75%|███████▍  | 56682/75722 [01:24<00:29, 654.22it/s] 75%|███████▍  | 56749/75722 [01:24<00:28, 658.29it/s] 75%|███████▌  | 56824/75722 [01:24<00:27, 683.95it/s] 75%|███████▌  | 56905/75722 [01:24<00:26, 719.64it/s] 75%|███████▌  | 56978/75722 [01:24<00:26, 711.93it/s] 75%|███████▌  | 57050/75722 [01:24<00:26, 714.11it/s] 75%|███████▌  | 57122/75722 [01:24<00:26, 701.19it/s] 76%|███████▌  | 57194/75722 [01:24<00:26, 704.96it/s] 76%|███████▌  | 57265/75722 [01:24<00:26, 694.96it/s] 76%|███████▌  | 57337/75722 [01:25<00:26, 700.89it/s] 76%|███████▌  | 57414/75722 [01:25<00:25, 718.91it/s] 76%|███████▌  | 57486/75722 [01:25<00:26, 681.51it/s] 76%|███████▌  | 57556/75722 [01:25<00:26, 685.70it/s] 76%|███████▌  | 57626/75722 [01:25<00:26, 688.07it/s] 76%|███████▌  | 57701/75722 [01:25<00:25, 705.83it/s] 76%|███████▋  | 57775/75722 [01:25<00:25, 713.37it/s] 76%|███████▋  | 57851/75722 [01:25<00:24, 725.35it/s] 76%|███████▋  | 57924/75722 [01:25<00:24, 725.00it/s] 77%|███████▋  | 57997/75722 [01:25<00:24, 724.84it/s] 77%|███████▋  | 58070/75722 [01:26<00:24, 711.26it/s] 77%|███████▋  | 58142/75722 [01:26<00:25, 698.94it/s] 77%|███████▋  | 58212/75722 [01:26<00:25, 696.45it/s] 77%|███████▋  | 58284/75722 [01:26<00:24, 700.11it/s] 77%|███████▋  | 58355/75722 [01:26<00:25, 689.06it/s] 77%|███████▋  | 58424/75722 [01:26<00:25, 685.73it/s] 77%|███████▋  | 58495/75722 [01:26<00:24, 692.26it/s] 77%|███████▋  | 58572/75722 [01:26<00:24, 712.32it/s] 77%|███████▋  | 58648/75722 [01:26<00:23, 726.17it/s] 78%|███████▊  | 58721/75722 [01:26<00:24, 704.46it/s] 78%|███████▊  | 58792/75722 [01:27<00:25, 676.22it/s] 78%|███████▊  | 58860/75722 [01:27<00:25, 666.67it/s] 78%|███████▊  | 58935/75722 [01:27<00:24, 688.85it/s] 78%|███████▊  | 59012/75722 [01:27<00:23, 710.20it/s] 78%|███████▊  | 59084/75722 [01:27<00:23, 697.13it/s] 78%|███████▊  | 59154/75722 [01:27<00:26, 624.72it/s] 78%|███████▊  | 59218/75722 [01:27<00:26, 621.94it/s] 78%|███████▊  | 59282/75722 [01:27<00:26, 617.84it/s] 78%|███████▊  | 59355/75722 [01:27<00:25, 649.17it/s] 78%|███████▊  | 59434/75722 [01:28<00:23, 687.41it/s] 79%|███████▊  | 59511/75722 [01:28<00:22, 709.01it/s] 79%|███████▊  | 59583/75722 [01:28<00:23, 691.25it/s] 79%|███████▉  | 59656/75722 [01:28<00:22, 701.59it/s] 79%|███████▉  | 59727/75722 [01:28<00:23, 677.60it/s] 79%|███████▉  | 59809/75722 [01:28<00:22, 716.64it/s] 79%|███████▉  | 59882/75722 [01:28<00:22, 708.55it/s] 79%|███████▉  | 59958/75722 [01:28<00:21, 720.97it/s] 79%|███████▉  | 60031/75722 [01:28<00:27, 576.03it/s] 79%|███████▉  | 60108/75722 [01:29<00:25, 624.17it/s] 79%|███████▉  | 60183/75722 [01:29<00:23, 655.62it/s] 80%|███████▉  | 60252/75722 [01:29<00:23, 653.06it/s] 80%|███████▉  | 60320/75722 [01:29<00:23, 642.99it/s] 80%|███████▉  | 60399/75722 [01:29<00:22, 681.98it/s] 80%|███████▉  | 60475/75722 [01:29<00:21, 703.14it/s] 80%|███████▉  | 60557/75722 [01:29<00:20, 735.85it/s] 80%|████████  | 60632/75722 [01:29<00:20, 723.80it/s] 80%|████████  | 60713/75722 [01:29<00:20, 748.29it/s] 80%|████████  | 60789/75722 [01:30<00:20, 745.86it/s] 80%|████████  | 60864/75722 [01:30<00:20, 738.71it/s] 80%|████████  | 60939/75722 [01:30<00:20, 724.18it/s] 81%|████████  | 61012/75722 [01:30<00:20, 704.16it/s] 81%|████████  | 61086/75722 [01:30<00:20, 713.20it/s] 81%|████████  | 61158/75722 [01:30<00:20, 706.02it/s] 81%|████████  | 61229/75722 [01:30<00:20, 694.43it/s] 81%|████████  | 61299/75722 [01:30<00:21, 681.95it/s] 81%|████████  | 61368/75722 [01:30<00:21, 677.69it/s] 81%|████████  | 61446/75722 [01:30<00:20, 704.92it/s] 81%|████████  | 61519/75722 [01:31<00:19, 711.49it/s] 81%|████████▏ | 61591/75722 [01:31<00:20, 703.05it/s] 81%|████████▏ | 61662/75722 [01:31<00:20, 674.08it/s] 82%|████████▏ | 61730/75722 [01:31<00:20, 669.00it/s] 82%|████████▏ | 61801/75722 [01:31<00:20, 680.62it/s] 82%|████████▏ | 61876/75722 [01:31<00:19, 700.10it/s] 82%|████████▏ | 61954/75722 [01:31<00:19, 723.08it/s] 82%|████████▏ | 62027/75722 [01:31<00:20, 680.93it/s] 82%|████████▏ | 62103/75722 [01:31<00:19, 702.08it/s] 82%|████████▏ | 62174/75722 [01:32<00:19, 698.10it/s] 82%|████████▏ | 62245/75722 [01:32<00:19, 678.80it/s] 82%|████████▏ | 62314/75722 [01:32<00:20, 662.45it/s] 82%|████████▏ | 62395/75722 [01:32<00:18, 703.35it/s] 83%|████████▎ | 62474/75722 [01:32<00:18, 726.76it/s] 83%|████████▎ | 62550/75722 [01:32<00:17, 735.73it/s] 83%|████████▎ | 62629/75722 [01:32<00:17, 748.48it/s] 83%|████████▎ | 62707/75722 [01:32<00:17, 756.42it/s] 83%|████████▎ | 62787/75722 [01:32<00:16, 768.27it/s] 83%|████████▎ | 62864/75722 [01:32<00:17, 746.89it/s] 83%|████████▎ | 62948/75722 [01:33<00:16, 772.37it/s] 83%|████████▎ | 63026/75722 [01:33<00:17, 741.03it/s] 83%|████████▎ | 63101/75722 [01:33<00:17, 728.14it/s] 83%|████████▎ | 63181/75722 [01:33<00:16, 747.53it/s] 84%|████████▎ | 63257/75722 [01:33<00:16, 740.00it/s] 84%|████████▎ | 63334/75722 [01:33<00:16, 745.90it/s] 84%|████████▎ | 63410/75722 [01:33<00:16, 749.79it/s] 84%|████████▍ | 63486/75722 [01:33<00:16, 752.78it/s] 84%|████████▍ | 63562/75722 [01:33<00:16, 741.42it/s] 84%|████████▍ | 63637/75722 [01:34<00:16, 727.90it/s] 84%|████████▍ | 63710/75722 [01:34<00:16, 726.79it/s] 84%|████████▍ | 63783/75722 [01:34<00:16, 709.36it/s] 84%|████████▍ | 63855/75722 [01:34<00:17, 676.98it/s] 84%|████████▍ | 63924/75722 [01:34<00:17, 663.07it/s] 85%|████████▍ | 63997/75722 [01:34<00:17, 670.44it/s] 85%|████████▍ | 64071/75722 [01:34<00:16, 688.87it/s] 85%|████████▍ | 64141/75722 [01:35<00:37, 308.80it/s] 85%|████████▍ | 64216/75722 [01:35<00:30, 376.80it/s] 85%|████████▍ | 64284/75722 [01:35<00:26, 430.65it/s] 85%|████████▍ | 64358/75722 [01:35<00:23, 493.32it/s] 85%|████████▌ | 64426/75722 [01:35<00:21, 534.47it/s] 85%|████████▌ | 64498/75722 [01:35<00:19, 578.10it/s] 85%|████████▌ | 64576/75722 [01:35<00:17, 629.82it/s] 85%|████████▌ | 64647/75722 [01:35<00:17, 651.05it/s] 85%|████████▌ | 64720/75722 [01:35<00:16, 672.50it/s] 86%|████████▌ | 64799/75722 [01:36<00:15, 705.08it/s] 86%|████████▌ | 64873/75722 [01:36<00:15, 695.08it/s] 86%|████████▌ | 64951/75722 [01:36<00:14, 719.11it/s] 86%|████████▌ | 65031/75722 [01:36<00:14, 741.04it/s] 86%|████████▌ | 65116/75722 [01:36<00:13, 770.42it/s] 86%|████████▌ | 65196/75722 [01:36<00:13, 779.03it/s] 86%|████████▌ | 65275/75722 [01:36<00:14, 737.24it/s] 86%|████████▋ | 65350/75722 [01:36<00:14, 711.68it/s] 86%|████████▋ | 65422/75722 [01:36<00:14, 689.81it/s] 86%|████████▋ | 65495/75722 [01:37<00:14, 700.21it/s] 87%|████████▋ | 65571/75722 [01:37<00:14, 715.45it/s] 87%|████████▋ | 65650/75722 [01:37<00:13, 734.57it/s] 87%|████████▋ | 65731/75722 [01:37<00:13, 753.27it/s] 87%|████████▋ | 65810/75722 [01:37<00:12, 763.63it/s] 87%|████████▋ | 65887/75722 [01:37<00:13, 737.57it/s] 87%|████████▋ | 65962/75722 [01:37<00:13, 739.82it/s] 87%|████████▋ | 66037/75722 [01:37<00:13, 719.99it/s] 87%|████████▋ | 66110/75722 [01:37<00:13, 718.99it/s] 87%|████████▋ | 66183/75722 [01:37<00:13, 705.99it/s] 87%|████████▋ | 66254/75722 [01:38<00:13, 706.84it/s] 88%|████████▊ | 66328/75722 [01:38<00:13, 714.89it/s] 88%|████████▊ | 66406/75722 [01:38<00:12, 733.44it/s] 88%|████████▊ | 66480/75722 [01:38<00:12, 717.28it/s] 88%|████████▊ | 66552/75722 [01:38<00:13, 680.89it/s] 88%|████████▊ | 66621/75722 [01:38<00:13, 678.03it/s] 88%|████████▊ | 66690/75722 [01:38<00:13, 671.40it/s] 88%|████████▊ | 66762/75722 [01:38<00:13, 685.28it/s] 88%|████████▊ | 66839/75722 [01:38<00:12, 708.10it/s] 88%|████████▊ | 66911/75722 [01:39<00:12, 690.35it/s] 88%|████████▊ | 66985/75722 [01:39<00:12, 703.48it/s] 89%|████████▊ | 67060/75722 [01:39<00:12, 715.14it/s] 89%|████████▊ | 67132/75722 [01:39<00:12, 710.91it/s] 89%|████████▉ | 67204/75722 [01:39<00:11, 712.61it/s] 89%|████████▉ | 67280/75722 [01:39<00:11, 724.24it/s] 89%|████████▉ | 67353/75722 [01:39<00:11, 721.05it/s] 89%|████████▉ | 67428/75722 [01:39<00:11, 728.54it/s] 89%|████████▉ | 67501/75722 [01:39<00:11, 728.46it/s] 89%|████████▉ | 67579/75722 [01:39<00:10, 743.18it/s] 89%|████████▉ | 67656/75722 [01:40<00:10, 750.31it/s] 89%|████████▉ | 67732/75722 [01:40<00:10, 739.97it/s] 90%|████████▉ | 67807/75722 [01:40<00:11, 717.50it/s] 90%|████████▉ | 67879/75722 [01:40<00:11, 681.37it/s] 90%|████████▉ | 67957/75722 [01:40<00:10, 706.80it/s] 90%|████████▉ | 68029/75722 [01:40<00:11, 697.61it/s] 90%|████████▉ | 68105/75722 [01:40<00:10, 714.98it/s] 90%|█████████ | 68189/75722 [01:40<00:10, 749.31it/s] 90%|█████████ | 68265/75722 [01:40<00:09, 747.95it/s] 90%|█████████ | 68340/75722 [01:40<00:09, 741.27it/s] 90%|█████████ | 68415/75722 [01:41<00:10, 709.54it/s] 90%|█████████ | 68502/75722 [01:41<00:09, 753.55it/s] 91%|█████████ | 68585/75722 [01:41<00:09, 773.77it/s] 91%|█████████ | 68668/75722 [01:41<00:08, 788.70it/s] 91%|█████████ | 68748/75722 [01:41<00:09, 764.52it/s] 91%|█████████ | 68825/75722 [01:41<00:09, 713.29it/s] 91%|█████████ | 68898/75722 [01:41<00:09, 686.47it/s] 91%|█████████ | 68975/75722 [01:41<00:09, 709.13it/s] 91%|█████████ | 69051/75722 [01:41<00:09, 721.67it/s] 91%|█████████▏| 69124/75722 [01:42<00:09, 718.50it/s] 91%|█████████▏| 69203/75722 [01:42<00:08, 738.55it/s] 91%|█████████▏| 69279/75722 [01:42<00:08, 742.86it/s] 92%|█████████▏| 69359/75722 [01:42<00:08, 756.49it/s] 92%|█████████▏| 69435/75722 [01:42<00:08, 739.77it/s] 92%|█████████▏| 69510/75722 [01:42<00:08, 704.82it/s] 92%|█████████▏| 69581/75722 [01:42<00:08, 684.78it/s] 92%|█████████▏| 69656/75722 [01:42<00:08, 700.95it/s] 92%|█████████▏| 69730/75722 [01:42<00:08, 710.04it/s] 92%|█████████▏| 69814/75722 [01:43<00:07, 747.55it/s] 92%|█████████▏| 69890/75722 [01:43<00:07, 738.64it/s] 92%|█████████▏| 69965/75722 [01:43<00:07, 731.79it/s] 92%|█████████▏| 70039/75722 [01:43<00:07, 731.89it/s] 93%|█████████▎| 70113/75722 [01:43<00:07, 718.43it/s] 93%|█████████▎| 70185/75722 [01:43<00:07, 705.87it/s] 93%|█████████▎| 70256/75722 [01:43<00:07, 701.40it/s] 93%|█████████▎| 70330/75722 [01:43<00:07, 711.95it/s] 93%|█████████▎| 70406/75722 [01:43<00:07, 721.71it/s] 93%|█████████▎| 70479/75722 [01:43<00:07, 658.66it/s] 93%|█████████▎| 70549/75722 [01:44<00:07, 668.20it/s] 93%|█████████▎| 70617/75722 [01:44<00:07, 654.56it/s] 93%|█████████▎| 70684/75722 [01:44<00:07, 630.39it/s] 93%|█████████▎| 70748/75722 [01:44<00:07, 631.74it/s] 94%|█████████▎| 70817/75722 [01:44<00:07, 645.54it/s] 94%|█████████▎| 70882/75722 [01:44<00:07, 629.81it/s] 94%|█████████▎| 70958/75722 [01:44<00:07, 664.36it/s] 94%|█████████▍| 71032/75722 [01:44<00:06, 686.09it/s] 94%|█████████▍| 71102/75722 [01:44<00:06, 687.54it/s] 94%|█████████▍| 71176/75722 [01:45<00:06, 688.40it/s] 94%|█████████▍| 71259/75722 [01:45<00:06, 728.05it/s] 94%|█████████▍| 71340/75722 [01:45<00:05, 751.13it/s] 94%|█████████▍| 71424/75722 [01:45<00:05, 776.99it/s] 94%|█████████▍| 71502/75722 [01:45<00:05, 735.94it/s] 95%|█████████▍| 71577/75722 [01:45<00:05, 735.74it/s] 95%|█████████▍| 71655/75722 [01:45<00:05, 746.56it/s] 95%|█████████▍| 71730/75722 [01:45<00:05, 729.02it/s] 95%|█████████▍| 71804/75722 [01:45<00:05, 696.50it/s] 95%|█████████▍| 71875/75722 [01:45<00:05, 695.34it/s] 95%|█████████▌| 71945/75722 [01:46<00:05, 682.62it/s] 95%|█████████▌| 72018/75722 [01:46<00:05, 692.05it/s] 95%|█████████▌| 72088/75722 [01:46<00:05, 666.82it/s] 95%|█████████▌| 72155/75722 [01:46<00:05, 666.10it/s] 95%|█████████▌| 72222/75722 [01:46<00:05, 643.39it/s] 95%|█████████▌| 72301/75722 [01:46<00:05, 682.98it/s] 96%|█████████▌| 72370/75722 [01:46<00:04, 676.40it/s] 96%|█████████▌| 72446/75722 [01:46<00:04, 697.36it/s] 96%|█████████▌| 72519/75722 [01:46<00:04, 704.37it/s] 96%|█████████▌| 72591/75722 [01:47<00:04, 707.30it/s] 96%|█████████▌| 72669/75722 [01:47<00:04, 727.00it/s] 96%|█████████▌| 72746/75722 [01:47<00:04, 737.13it/s] 96%|█████████▌| 72827/75722 [01:47<00:03, 758.14it/s] 96%|█████████▋| 72908/75722 [01:47<00:03, 771.03it/s] 96%|█████████▋| 72986/75722 [01:47<00:03, 738.46it/s] 96%|█████████▋| 73064/75722 [01:47<00:03, 747.02it/s] 97%|█████████▋| 73140/75722 [01:47<00:03, 750.75it/s] 97%|█████████▋| 73222/75722 [01:47<00:03, 770.99it/s] 97%|█████████▋| 73300/75722 [01:47<00:03, 766.11it/s] 97%|█████████▋| 73377/75722 [01:48<00:03, 744.37it/s] 97%|█████████▋| 73453/75722 [01:48<00:03, 745.91it/s] 97%|█████████▋| 73528/75722 [01:48<00:03, 724.60it/s] 97%|█████████▋| 73601/75722 [01:48<00:03, 662.45it/s] 97%|█████████▋| 73677/75722 [01:48<00:02, 687.01it/s] 97%|█████████▋| 73751/75722 [01:48<00:02, 701.09it/s] 97%|█████████▋| 73825/75722 [01:48<00:02, 710.56it/s] 98%|█████████▊| 73897/75722 [01:48<00:02, 690.07it/s] 98%|█████████▊| 73978/75722 [01:48<00:02, 723.57it/s] 98%|█████████▊| 74064/75722 [01:49<00:02, 761.24it/s] 98%|█████████▊| 74144/75722 [01:49<00:02, 769.76it/s] 98%|█████████▊| 74222/75722 [01:49<00:01, 770.77it/s] 98%|█████████▊| 74304/75722 [01:49<00:01, 783.12it/s] 98%|█████████▊| 74383/75722 [01:49<00:01, 760.22it/s] 98%|█████████▊| 74460/75722 [01:49<00:01, 730.45it/s] 98%|█████████▊| 74534/75722 [01:49<00:01, 702.08it/s] 99%|█████████▊| 74605/75722 [01:49<00:01, 685.37it/s] 99%|█████████▊| 74677/75722 [01:49<00:01, 694.66it/s] 99%|█████████▊| 74752/75722 [01:49<00:01, 710.05it/s] 99%|█████████▉| 74828/75722 [01:50<00:01, 721.87it/s] 99%|█████████▉| 74901/75722 [01:50<00:01, 711.88it/s] 99%|█████████▉| 74973/75722 [01:50<00:01, 711.06it/s] 99%|█████████▉| 75045/75722 [01:50<00:00, 682.81it/s] 99%|█████████▉| 75118/75722 [01:50<00:00, 695.99it/s] 99%|█████████▉| 75194/75722 [01:50<00:00, 712.75it/s] 99%|█████████▉| 75277/75722 [01:50<00:00, 745.45it/s]100%|█████████▉| 75352/75722 [01:50<00:00, 731.55it/s]100%|█████████▉| 75438/75722 [01:50<00:00, 767.36it/s]100%|█████████▉| 75515/75722 [01:51<00:00, 752.66it/s]100%|█████████▉| 75591/75722 [01:51<00:00, 734.77it/s]100%|█████████▉| 75665/75722 [01:51<00:00, 731.49it/s]100%|██████████| 75722/75722 [01:51<00:00, 680.34it/s]
2023-08-25 09:13:06 INFO     after remove the overflow : 75195
2023-08-25 09:13:13 INFO     preprocessed feature is saved at /home/dalyasin/.cache/lmqg/encoded_featurelmqg/qg_squad/t5-base.512.32.paragraph_answer.question.train.None.pkl
2023-08-25 09:13:13 INFO     start model training
Traceback (most recent call last):
  File "/home/dalyasin/miniconda3/envs/lmqg/bin/lmqg-train-search", line 8, in <module>
    sys.exit(main_training_search())
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/lmqg/lmqg_cl/model_finetuning.py", line 138, in main_training_search
    trainer.run(interval=opt.interval, overwrite=opt.overwrite)
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/lmqg/grid_searcher.py", line 215, in run
    trainer.train(
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/lmqg/trainer.py", line 236, in train
    mean_loss, global_step = self.train_single_epoch(loader, global_step, interval)
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/lmqg/trainer.py", line 256, in train_single_epoch
    loss = self.model.encode_to_loss(encode)
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/lmqg/language_model.py", line 668, in encode_to_loss
    output = self.model(**{k: v.to(self.device) for k, v in encode.items()})
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 171, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 181, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 89, in parallel_apply
    output.reraise()
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/_utils.py", line 644, in reraise
    raise exception
torch.cuda.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 64, in _worker
    output = module(*input, **kwargs)
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 1680, in forward
    encoder_outputs = self.encoder(
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 1094, in forward
    layer_outputs = layer_module(
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 694, in forward
    self_attention_outputs = self.layer[0](
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 601, in forward
    attention_output = self.SelfAttention(
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 564, in forward
    attn_weights = nn.functional.dropout(
  File "/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 47.54 GiB total capacity; 9.85 GiB already allocated; 86.69 MiB free; 9.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

nohup: ignoring input
2023-08-25 09:20:45 INFO     INITIALIZE GRID SEARCHER: 12 configs to try
2023-08-25 09:20:45 INFO     ## 1st RUN: Configuration 0/12 ##
2023-08-25 09:20:45 INFO     skip as the config exists at ['tmp_ckpt_t5-base/model_avxiio'] 
{'dataset_path': 'lmqg/qg_squad', 'dataset_name': 'default', 'input_types': 'paragraph_answer', 'output_types': 'question', 'model': 't5-base', 'fp16': False, 'batch': 64, 'epoch': 15, 'max_length': 512, 'max_length_output': 32, 'prefix_types': None, 'lr': 0.001, 'label_smoothing': 0.15, 'random_seed': 1, 'gradient_accumulation_steps': 4}
2023-08-25 09:20:45 INFO     initialize model trainer
2023-08-25 09:20:45 INFO     load config from existing checkpoint at tmp_ckpt_t5-base/model_avxiio
2023-08-25 09:20:45 INFO     hyperparameters
2023-08-25 09:20:45 INFO     	 * dataset_path: lmqg/qg_squad
2023-08-25 09:20:45 INFO     	 * dataset_name: default
2023-08-25 09:20:45 INFO     	 * input_types: paragraph_answer
2023-08-25 09:20:45 INFO     	 * output_types: question
2023-08-25 09:20:45 INFO     	 * prefix_types: None
2023-08-25 09:20:45 INFO     	 * model: t5-base
2023-08-25 09:20:45 INFO     	 * max_length: 512
2023-08-25 09:20:45 INFO     	 * max_length_output: 32
2023-08-25 09:20:45 INFO     	 * epoch: 15
2023-08-25 09:20:45 INFO     	 * batch: 64
2023-08-25 09:20:45 INFO     	 * lr: 0.001
2023-08-25 09:20:45 INFO     	 * fp16: False
2023-08-25 09:20:45 INFO     	 * random_seed: 1
2023-08-25 09:20:45 INFO     	 * gradient_accumulation_steps: 4
2023-08-25 09:20:45 INFO     	 * label_smoothing: 0.15
2023-08-25 09:20:45 INFO     initialize checkpoint with t5-base
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
2023-08-25 09:20:47 INFO     Created a temporary directory at /tmp/tmp349dl9py
2023-08-25 09:20:47 INFO     Writing /tmp/tmp349dl9py/_remote_module_non_scriptable.py
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
2023-08-25 09:20:51 INFO     use spaCy answer extraction model: positionrank
2023-08-25 09:20:53 INFO     Model `t5-base`
2023-08-25 09:20:53 INFO     	 * Num of GPU in use: 2
2023-08-25 09:20:53 INFO     	 * Prefix: False
2023-08-25 09:20:53 INFO     	 * Language: en (ignore at the training phase)
2023-08-25 09:20:53 INFO     dataset preprocessing
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=False' instead.
  warnings.warn(
2023-08-25 09:20:56 INFO     loading preprocessed feature from /home/dalyasin/.cache/lmqg/encoded_featurelmqg/qg_squad/t5-base.512.32.paragraph_answer.question.train.None.pkl
2023-08-25 09:21:12 INFO     start model training
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-25 09:25:51 INFO     	 * (global step 50: loss: 0.8082338571548462, lr: 0.001
2023-08-25 09:30:30 INFO     	 * (global step 100: loss: 0.6930474787950516, lr: 0.001
2023-08-25 09:35:07 INFO     	 * (global step 150: loss: 0.71214759349823, lr: 0.001
2023-08-25 09:39:46 INFO     	 * (global step 200: loss: 0.6986940205097198, lr: 0.001
2023-08-25 09:44:24 INFO     	 * (global step 250: loss: 0.6850151270627975, lr: 0.001
2023-08-25 09:48:26 INFO     [epoch 0/15] average loss: 0.833, lr: 0.001
2023-08-25 09:48:26 INFO     saving model related files
2023-08-25 09:48:26 INFO     saving model
2023-08-25 09:48:27 INFO     saving tokenizer
2023-08-25 09:48:27 INFO     saving optimizer
2023-08-25 09:48:30 INFO     remove old optimizer files
2023-08-25 09:49:09 INFO     	 * (global step 300: loss: 0.6216619461774826, lr: 0.001
2023-08-25 09:53:46 INFO     	 * (global step 350: loss: 0.6098613142967224, lr: 0.001
2023-08-25 09:58:21 INFO     	 * (global step 400: loss: 0.6289360970258713, lr: 0.001
2023-08-25 10:03:00 INFO     	 * (global step 450: loss: 0.6119928508996964, lr: 0.001
2023-08-25 10:07:37 INFO     	 * (global step 500: loss: 0.6143195331096649, lr: 0.001
2023-08-25 10:12:16 INFO     	 * (global step 550: loss: 0.6306560039520264, lr: 0.001
2023-08-25 10:15:38 INFO     [epoch 1/15] average loss: 0.604, lr: 0.001
2023-08-25 10:15:38 INFO     saving model related files
2023-08-25 10:15:38 INFO     saving model
2023-08-25 10:15:39 INFO     saving tokenizer
2023-08-25 10:15:39 INFO     saving optimizer
2023-08-25 10:15:41 INFO     remove old optimizer files
2023-08-25 10:16:59 INFO     	 * (global step 600: loss: 0.5544241741299629, lr: 0.001
2023-08-25 10:21:36 INFO     	 * (global step 650: loss: 0.5059192627668381, lr: 0.001
2023-08-25 10:26:12 INFO     	 * (global step 700: loss: 0.5357641577720642, lr: 0.001
2023-08-25 10:30:49 INFO     	 * (global step 750: loss: 0.514945462346077, lr: 0.001
2023-08-25 10:35:27 INFO     	 * (global step 800: loss: 0.5290607213973999, lr: 0.001
2023-08-25 10:40:03 INFO     	 * (global step 850: loss: 0.4921325519680977, lr: 0.001
2023-08-25 10:42:48 INFO     [epoch 2/15] average loss: 0.53, lr: 0.001
2023-08-25 10:42:48 INFO     saving model related files
2023-08-25 10:42:48 INFO     saving model
2023-08-25 10:42:49 INFO     saving tokenizer
2023-08-25 10:42:49 INFO     saving optimizer
2023-08-25 10:42:52 INFO     remove old optimizer files
2023-08-25 10:44:48 INFO     	 * (global step 900: loss: 0.48346128314733505, lr: 0.001
2023-08-25 10:49:26 INFO     	 * (global step 950: loss: 0.48042890429496765, lr: 0.001
2023-08-25 10:54:02 INFO     	 * (global step 1000: loss: 0.4587213173508644, lr: 0.001
2023-08-25 10:58:38 INFO     	 * (global step 1050: loss: 0.46109639108181, lr: 0.001
2023-08-25 11:03:15 INFO     	 * (global step 1100: loss: 0.48664791136980057, lr: 0.001
2023-08-25 11:07:52 INFO     	 * (global step 1150: loss: 0.4758119657635689, lr: 0.001
2023-08-25 11:09:58 INFO     [epoch 3/15] average loss: 0.472, lr: 0.001
2023-08-25 11:09:58 INFO     saving model related files
2023-08-25 11:09:58 INFO     saving model
2023-08-25 11:09:59 INFO     saving tokenizer
2023-08-25 11:09:59 INFO     saving optimizer
2023-08-25 11:10:01 INFO     remove old optimizer files
2023-08-25 11:12:37 INFO     	 * (global step 1200: loss: 0.3877866640686989, lr: 0.001
2023-08-25 11:17:13 INFO     	 * (global step 1250: loss: 0.3870493620634079, lr: 0.001
2023-08-25 11:21:50 INFO     	 * (global step 1300: loss: 0.44503380358219147, lr: 0.001
2023-08-25 11:26:27 INFO     	 * (global step 1350: loss: 0.4424983710050583, lr: 0.001
2023-08-25 11:31:04 INFO     	 * (global step 1400: loss: 0.44696760922670364, lr: 0.001
2023-08-25 11:35:41 INFO     	 * (global step 1450: loss: 0.4645703136920929, lr: 0.001
2023-08-25 11:37:07 INFO     [epoch 4/15] average loss: 0.422, lr: 0.001
2023-08-25 11:37:07 INFO     saving model related files
2023-08-25 11:37:07 INFO     saving model
2023-08-25 11:37:08 INFO     saving tokenizer
2023-08-25 11:37:08 INFO     saving optimizer
2023-08-25 11:37:11 INFO     remove old optimizer files
2023-08-25 11:37:11 INFO     complete training: model ckpt was saved at tmp_ckpt_t5-base/model_avxiio
2023-08-25 11:37:14 INFO     ## 1st RUN: Configuration 1/12 ##
2023-08-25 11:37:14 INFO     initialize model trainer
2023-08-25 11:37:14 INFO     initialize checkpoint at tmp_ckpt_t5-base/model_eszyci
2023-08-25 11:37:14 INFO     hyperparameters
2023-08-25 11:37:14 INFO     	 * dataset_path: lmqg/qg_squad
2023-08-25 11:37:14 INFO     	 * dataset_name: default
2023-08-25 11:37:14 INFO     	 * input_types: paragraph_answer
2023-08-25 11:37:14 INFO     	 * output_types: question
2023-08-25 11:37:14 INFO     	 * prefix_types: None
2023-08-25 11:37:14 INFO     	 * model: t5-base
2023-08-25 11:37:14 INFO     	 * max_length: 512
2023-08-25 11:37:14 INFO     	 * max_length_output: 32
2023-08-25 11:37:14 INFO     	 * epoch: 15
2023-08-25 11:37:14 INFO     	 * batch: 64
2023-08-25 11:37:14 INFO     	 * lr: 0.001
2023-08-25 11:37:14 INFO     	 * fp16: False
2023-08-25 11:37:14 INFO     	 * random_seed: 1
2023-08-25 11:37:14 INFO     	 * gradient_accumulation_steps: 2
2023-08-25 11:37:14 INFO     	 * label_smoothing: 0.15
2023-08-25 11:37:14 INFO     initialize checkpoint with t5-base
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
/home/dalyasin/miniconda3/envs/lmqg/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
2023-08-25 11:37:19 INFO     use spaCy answer extraction model: positionrank
2023-08-25 11:37:20 INFO     Model `t5-base`
2023-08-25 11:37:20 INFO     	 * Num of GPU in use: 2
2023-08-25 11:37:20 INFO     	 * Prefix: False
2023-08-25 11:37:20 INFO     	 * Language: en (ignore at the training phase)
2023-08-25 11:37:20 INFO     dataset preprocessing
2023-08-25 11:37:22 INFO     loading preprocessed feature from /home/dalyasin/.cache/lmqg/encoded_featurelmqg/qg_squad/t5-base.512.32.paragraph_answer.question.train.None.pkl
2023-08-25 11:37:36 INFO     start model training
2023-08-25 11:39:55 INFO     	 * (global step 50: loss: 0.7979540526866913, lr: 0.001
2023-08-25 11:42:14 INFO     	 * (global step 100: loss: 0.6931226253509521, lr: 0.001
2023-08-25 11:44:34 INFO     	 * (global step 150: loss: 0.7404115796089172, lr: 0.001
2023-08-25 11:46:52 INFO     	 * (global step 200: loss: 0.7208672761917114, lr: 0.001
2023-08-25 11:49:11 INFO     	 * (global step 250: loss: 0.7515690326690674, lr: 0.001
2023-08-25 11:51:30 INFO     	 * (global step 300: loss: 0.717822790145874, lr: 0.001
2023-08-25 11:53:49 INFO     	 * (global step 350: loss: 0.6113399863243103, lr: 0.001
2023-08-25 11:56:08 INFO     	 * (global step 400: loss: 0.6368691623210907, lr: 0.001
2023-08-25 11:58:29 INFO     	 * (global step 450: loss: 0.6343205571174622, lr: 0.001
2023-08-25 12:00:48 INFO     	 * (global step 500: loss: 0.7288486361503601, lr: 0.001
2023-08-25 12:03:09 INFO     	 * (global step 550: loss: 0.6290831863880157, lr: 0.001
2023-08-25 12:04:52 INFO     [epoch 0/15] average loss: 0.77, lr: 0.001
2023-08-25 12:04:52 INFO     saving model related files
2023-08-25 12:04:52 INFO     saving model
2023-08-25 12:04:53 INFO     saving tokenizer
2023-08-25 12:04:53 INFO     saving optimizer
2023-08-25 12:04:56 INFO     remove old optimizer files
2023-08-25 12:05:33 INFO     	 * (global step 600: loss: 0.6017974615097046, lr: 0.001
2023-08-25 12:07:51 INFO     	 * (global step 650: loss: 0.6039931178092957, lr: 0.001
2023-08-25 12:10:10 INFO     	 * (global step 700: loss: 0.5816770195960999, lr: 0.001
2023-08-25 12:12:29 INFO     	 * (global step 750: loss: 0.6185030341148376, lr: 0.001
2023-08-25 12:14:48 INFO     	 * (global step 800: loss: 0.6231783628463745, lr: 0.001
2023-08-25 12:17:08 INFO     	 * (global step 850: loss: 0.6113907992839813, lr: 0.001
2023-08-25 12:19:27 INFO     	 * (global step 900: loss: 0.5992139577865601, lr: 0.001
2023-08-25 12:21:46 INFO     	 * (global step 950: loss: 0.5927358865737915, lr: 0.001
2023-08-25 12:24:05 INFO     	 * (global step 1000: loss: 0.5520282089710236, lr: 0.001
2023-08-25 12:26:25 INFO     	 * (global step 1050: loss: 0.5351412892341614, lr: 0.001
2023-08-25 12:28:44 INFO     	 * (global step 1100: loss: 0.6310164332389832, lr: 0.001
2023-08-25 12:31:03 INFO     	 * (global step 1150: loss: 0.556897759437561, lr: 0.001
2023-08-25 12:32:10 INFO     [epoch 1/15] average loss: 0.591, lr: 0.001
2023-08-25 12:32:10 INFO     saving model related files
2023-08-25 12:32:10 INFO     saving model
2023-08-25 12:32:12 INFO     saving tokenizer
2023-08-25 12:32:12 INFO     saving optimizer
2023-08-25 12:32:14 INFO     remove old optimizer files
2023-08-25 12:33:27 INFO     	 * (global step 1200: loss: 0.4980151057243347, lr: 0.001
2023-08-25 12:35:45 INFO     	 * (global step 1250: loss: 0.5711027681827545, lr: 0.001
2023-08-25 12:38:05 INFO     	 * (global step 1300: loss: 0.4599839150905609, lr: 0.001
2023-08-25 12:40:24 INFO     	 * (global step 1350: loss: 0.5119042247533798, lr: 0.001
2023-08-25 12:42:43 INFO     	 * (global step 1400: loss: 0.4684712290763855, lr: 0.001
2023-08-25 12:45:02 INFO     	 * (global step 1450: loss: 0.5089607983827591, lr: 0.001
2023-08-25 12:47:21 INFO     	 * (global step 1500: loss: 0.48688018321990967, lr: 0.001
2023-08-25 12:49:40 INFO     	 * (global step 1550: loss: 0.5215549468994141, lr: 0.001
2023-08-25 12:51:59 INFO     	 * (global step 1600: loss: 0.5126292854547501, lr: 0.001
2023-08-25 12:54:18 INFO     	 * (global step 1650: loss: 0.4913572072982788, lr: 0.001
2023-08-25 12:56:38 INFO     	 * (global step 1700: loss: 0.5042532980442047, lr: 0.001
2023-08-25 12:58:57 INFO     	 * (global step 1750: loss: 0.5748167037963867, lr: 0.001
2023-08-25 12:59:28 INFO     [epoch 2/15] average loss: 0.517, lr: 0.001
2023-08-25 12:59:28 INFO     saving model related files
2023-08-25 12:59:28 INFO     saving model
2023-08-25 12:59:29 INFO     saving tokenizer
2023-08-25 12:59:29 INFO     saving optimizer
2023-08-25 12:59:31 INFO     remove old optimizer files
2023-08-25 13:01:21 INFO     	 * (global step 1800: loss: 0.4128561317920685, lr: 0.001
2023-08-25 13:03:39 INFO     	 * (global step 1850: loss: 0.41349753737449646, lr: 0.001
2023-08-25 13:05:57 INFO     	 * (global step 1900: loss: 0.40168339014053345, lr: 0.001
2023-08-25 13:08:16 INFO     	 * (global step 1950: loss: 0.4523126631975174, lr: 0.001
2023-08-25 13:10:34 INFO     	 * (global step 2000: loss: 0.4391903430223465, lr: 0.001
2023-08-25 13:12:53 INFO     	 * (global step 2050: loss: 0.4490729868412018, lr: 0.001
2023-08-25 13:15:12 INFO     	 * (global step 2100: loss: 0.4547843933105469, lr: 0.001
2023-08-25 13:17:31 INFO     	 * (global step 2150: loss: 0.49062561988830566, lr: 0.001
2023-08-25 13:19:49 INFO     	 * (global step 2200: loss: 0.47167475521564484, lr: 0.001
2023-08-25 13:22:08 INFO     	 * (global step 2250: loss: 0.46483537554740906, lr: 0.001
2023-08-25 13:24:26 INFO     	 * (global step 2300: loss: 0.44864848256111145, lr: 0.001
2023-08-25 13:26:40 INFO     [epoch 3/15] average loss: 0.453, lr: 0.001
2023-08-25 13:26:40 INFO     saving model related files
2023-08-25 13:26:40 INFO     saving model
2023-08-25 13:26:41 INFO     saving tokenizer
2023-08-25 13:26:41 INFO     saving optimizer
2023-08-25 13:26:43 INFO     remove old optimizer files
2023-08-25 13:26:49 INFO     	 * (global step 2350: loss: 0.37037715315818787, lr: 0.001
2023-08-25 13:29:08 INFO     	 * (global step 2400: loss: 0.3552447408437729, lr: 0.001
2023-08-25 13:31:27 INFO     	 * (global step 2450: loss: 0.3889964520931244, lr: 0.001
2023-08-25 13:33:45 INFO     	 * (global step 2500: loss: 0.41315844655036926, lr: 0.001
2023-08-25 13:36:04 INFO     	 * (global step 2550: loss: 0.3846043050289154, lr: 0.001
2023-08-25 13:38:23 INFO     	 * (global step 2600: loss: 0.4308590441942215, lr: 0.001
2023-08-25 13:40:41 INFO     	 * (global step 2650: loss: 0.40503619611263275, lr: 0.001
2023-08-25 13:43:00 INFO     	 * (global step 2700: loss: 0.41489480435848236, lr: 0.001
2023-08-25 13:45:19 INFO     	 * (global step 2750: loss: 0.3966265022754669, lr: 0.001
2023-08-25 13:47:37 INFO     	 * (global step 2800: loss: 0.38761743903160095, lr: 0.001
2023-08-25 13:49:56 INFO     	 * (global step 2850: loss: 0.41950763761997223, lr: 0.001
2023-08-25 13:52:16 INFO     	 * (global step 2900: loss: 0.40585386753082275, lr: 0.001
2023-08-25 13:53:53 INFO     [epoch 4/15] average loss: 0.399, lr: 0.001
2023-08-25 13:53:53 INFO     saving model related files
2023-08-25 13:53:53 INFO     saving model
2023-08-25 13:53:54 INFO     saving tokenizer
2023-08-25 13:53:54 INFO     saving optimizer
2023-08-25 13:53:57 INFO     remove old optimizer files
2023-08-25 13:53:57 INFO     complete training: model ckpt was saved at tmp_ckpt_t5-base/model_eszyci
2023-08-25 13:54:00 INFO     ## 1st RUN: Configuration 2/12 ##
2023-08-25 13:54:00 INFO     initialize model trainer
2023-08-25 13:54:00 INFO     initialize checkpoint at tmp_ckpt_t5-base/model_dpyopu
2023-08-25 13:54:00 INFO     hyperparameters
2023-08-25 13:54:00 INFO     	 * dataset_path: lmqg/qg_squad
2023-08-25 13:54:00 INFO     	 * dataset_name: default
2023-08-25 13:54:00 INFO     	 * input_types: paragraph_answer
2023-08-25 13:54:00 INFO     	 * output_types: question
2023-08-25 13:54:00 INFO     	 * prefix_types: None
2023-08-25 13:54:00 INFO     	 * model: t5-base
2023-08-25 13:54:00 INFO     	 * max_length: 512
2023-08-25 13:54:00 INFO     	 * max_length_output: 32
2023-08-25 13:54:00 INFO     	 * epoch: 15
2023-08-25 13:54:00 INFO     	 * batch: 64
2023-08-25 13:54:00 INFO     	 * lr: 0.001
2023-08-25 13:54:00 INFO     	 * fp16: False
2023-08-25 13:54:00 INFO     	 * random_seed: 1
2023-08-25 13:54:00 INFO     	 * gradient_accumulation_steps: 4
2023-08-25 13:54:00 INFO     	 * label_smoothing: 0.0
2023-08-25 13:54:00 INFO     initialize checkpoint with t5-base
2023-08-25 13:54:06 INFO     use spaCy answer extraction model: positionrank
2023-08-25 13:54:06 INFO     Model `t5-base`
2023-08-25 13:54:06 INFO     	 * Num of GPU in use: 2
2023-08-25 13:54:06 INFO     	 * Prefix: False
2023-08-25 13:54:06 INFO     	 * Language: en (ignore at the training phase)
2023-08-25 13:54:06 INFO     dataset preprocessing
2023-08-25 13:54:09 INFO     loading preprocessed feature from /home/dalyasin/.cache/lmqg/encoded_featurelmqg/qg_squad/t5-base.512.32.paragraph_answer.question.train.None.pkl
2023-08-25 13:54:21 INFO     start model training
2023-08-25 13:58:58 INFO     	 * (global step 50: loss: 0.8082338571548462, lr: 0.001
2023-08-25 14:03:38 INFO     	 * (global step 100: loss: 0.6930474787950516, lr: 0.001
2023-08-25 14:08:16 INFO     	 * (global step 150: loss: 0.71214759349823, lr: 0.001
2023-08-25 14:12:54 INFO     	 * (global step 200: loss: 0.6986940205097198, lr: 0.001
2023-08-25 14:17:31 INFO     	 * (global step 250: loss: 0.6850151270627975, lr: 0.001
2023-08-25 14:21:33 INFO     [epoch 0/15] average loss: 0.833, lr: 0.001
2023-08-25 14:21:33 INFO     saving model related files
2023-08-25 14:21:33 INFO     saving model
2023-08-25 14:21:35 INFO     saving tokenizer
2023-08-25 14:21:35 INFO     saving optimizer
2023-08-25 14:21:37 INFO     remove old optimizer files
2023-08-25 14:22:15 INFO     	 * (global step 300: loss: 0.6216619461774826, lr: 0.001
2023-08-25 14:26:53 INFO     	 * (global step 350: loss: 0.6098613142967224, lr: 0.001
2023-08-25 14:31:29 INFO     	 * (global step 400: loss: 0.6289360970258713, lr: 0.001
2023-08-25 14:36:05 INFO     	 * (global step 450: loss: 0.6119928508996964, lr: 0.001
2023-08-25 14:40:42 INFO     	 * (global step 500: loss: 0.6143195331096649, lr: 0.001
2023-08-25 14:45:19 INFO     	 * (global step 550: loss: 0.6306560039520264, lr: 0.001
2023-08-25 14:48:41 INFO     [epoch 1/15] average loss: 0.604, lr: 0.001
2023-08-25 14:48:41 INFO     saving model related files
2023-08-25 14:48:41 INFO     saving model
2023-08-25 14:48:42 INFO     saving tokenizer
2023-08-25 14:48:42 INFO     saving optimizer
2023-08-25 14:48:45 INFO     remove old optimizer files
2023-08-25 14:50:02 INFO     	 * (global step 600: loss: 0.5544241741299629, lr: 0.001
2023-08-25 14:54:42 INFO     	 * (global step 650: loss: 0.5059192627668381, lr: 0.001
2023-08-25 14:59:19 INFO     	 * (global step 700: loss: 0.5357641577720642, lr: 0.001
2023-08-25 15:03:56 INFO     	 * (global step 750: loss: 0.514945462346077, lr: 0.001
2023-08-25 15:08:35 INFO     	 * (global step 800: loss: 0.5290607213973999, lr: 0.001
2023-08-25 15:13:12 INFO     	 * (global step 850: loss: 0.4921325519680977, lr: 0.001
2023-08-25 15:15:56 INFO     [epoch 2/15] average loss: 0.53, lr: 0.001
2023-08-25 15:15:56 INFO     saving model related files
2023-08-25 15:15:56 INFO     saving model
2023-08-25 15:15:57 INFO     saving tokenizer
2023-08-25 15:15:57 INFO     saving optimizer
2023-08-25 15:16:00 INFO     remove old optimizer files
2023-08-25 15:17:56 INFO     	 * (global step 900: loss: 0.48346128314733505, lr: 0.001
2023-08-25 15:22:34 INFO     	 * (global step 950: loss: 0.48042890429496765, lr: 0.001
2023-08-25 15:27:11 INFO     	 * (global step 1000: loss: 0.4587213173508644, lr: 0.001
2023-08-25 15:31:47 INFO     	 * (global step 1050: loss: 0.46109639108181, lr: 0.001
2023-08-25 15:36:25 INFO     	 * (global step 1100: loss: 0.48664791136980057, lr: 0.001
2023-08-25 15:41:02 INFO     	 * (global step 1150: loss: 0.4758119657635689, lr: 0.001
2023-08-25 15:43:06 INFO     [epoch 3/15] average loss: 0.472, lr: 0.001
2023-08-25 15:43:06 INFO     saving model related files
2023-08-25 15:43:06 INFO     saving model
2023-08-25 15:43:07 INFO     saving tokenizer
2023-08-25 15:43:07 INFO     saving optimizer
2023-08-25 15:43:10 INFO     remove old optimizer files
2023-08-25 15:45:46 INFO     	 * (global step 1200: loss: 0.3877866640686989, lr: 0.001
2023-08-25 15:50:22 INFO     	 * (global step 1250: loss: 0.3870493620634079, lr: 0.001
2023-08-25 15:54:59 INFO     	 * (global step 1300: loss: 0.44503380358219147, lr: 0.001
2023-08-25 15:59:35 INFO     	 * (global step 1350: loss: 0.4424983710050583, lr: 0.001
2023-08-25 16:04:12 INFO     	 * (global step 1400: loss: 0.44696760922670364, lr: 0.001
2023-08-25 16:08:48 INFO     	 * (global step 1450: loss: 0.4645703136920929, lr: 0.001
2023-08-25 16:10:14 INFO     [epoch 4/15] average loss: 0.422, lr: 0.001
2023-08-25 16:10:14 INFO     saving model related files
2023-08-25 16:10:14 INFO     saving model
2023-08-25 16:10:15 INFO     saving tokenizer
2023-08-25 16:10:15 INFO     saving optimizer
2023-08-25 16:10:17 INFO     remove old optimizer files
2023-08-25 16:10:18 INFO     complete training: model ckpt was saved at tmp_ckpt_t5-base/model_dpyopu
2023-08-25 16:10:21 INFO     ## 1st RUN: Configuration 3/12 ##
2023-08-25 16:10:21 INFO     initialize model trainer
2023-08-25 16:10:21 INFO     initialize checkpoint at tmp_ckpt_t5-base/model_mzgdpa
2023-08-25 16:10:21 INFO     hyperparameters
2023-08-25 16:10:21 INFO     	 * dataset_path: lmqg/qg_squad
2023-08-25 16:10:21 INFO     	 * dataset_name: default
2023-08-25 16:10:21 INFO     	 * input_types: paragraph_answer
2023-08-25 16:10:21 INFO     	 * output_types: question
2023-08-25 16:10:21 INFO     	 * prefix_types: None
2023-08-25 16:10:21 INFO     	 * model: t5-base
2023-08-25 16:10:21 INFO     	 * max_length: 512
2023-08-25 16:10:21 INFO     	 * max_length_output: 32
2023-08-25 16:10:21 INFO     	 * epoch: 15
2023-08-25 16:10:21 INFO     	 * batch: 64
2023-08-25 16:10:21 INFO     	 * lr: 0.001
2023-08-25 16:10:21 INFO     	 * fp16: False
2023-08-25 16:10:21 INFO     	 * random_seed: 1
2023-08-25 16:10:21 INFO     	 * gradient_accumulation_steps: 2
2023-08-25 16:10:21 INFO     	 * label_smoothing: 0.0
2023-08-25 16:10:21 INFO     initialize checkpoint with t5-base
2023-08-25 16:10:25 INFO     use spaCy answer extraction model: positionrank
2023-08-25 16:10:26 INFO     Model `t5-base`
2023-08-25 16:10:26 INFO     	 * Num of GPU in use: 2
2023-08-25 16:10:26 INFO     	 * Prefix: False
2023-08-25 16:10:26 INFO     	 * Language: en (ignore at the training phase)
2023-08-25 16:10:26 INFO     dataset preprocessing
2023-08-25 16:10:28 INFO     loading preprocessed feature from /home/dalyasin/.cache/lmqg/encoded_featurelmqg/qg_squad/t5-base.512.32.paragraph_answer.question.train.None.pkl
2023-08-25 16:10:41 INFO     start model training
2023-08-25 16:13:01 INFO     	 * (global step 50: loss: 0.7979540526866913, lr: 0.001
2023-08-25 16:15:19 INFO     	 * (global step 100: loss: 0.6931226253509521, lr: 0.001
2023-08-25 16:17:39 INFO     	 * (global step 150: loss: 0.7404115796089172, lr: 0.001
2023-08-25 16:19:58 INFO     	 * (global step 200: loss: 0.7208672761917114, lr: 0.001
2023-08-25 16:22:16 INFO     	 * (global step 250: loss: 0.7515690326690674, lr: 0.001
2023-08-25 16:24:36 INFO     	 * (global step 300: loss: 0.717822790145874, lr: 0.001
2023-08-25 16:26:56 INFO     	 * (global step 350: loss: 0.6113399863243103, lr: 0.001
2023-08-25 16:29:14 INFO     	 * (global step 400: loss: 0.6368691623210907, lr: 0.001
2023-08-25 16:31:34 INFO     	 * (global step 450: loss: 0.6343205571174622, lr: 0.001
2023-08-25 16:33:54 INFO     	 * (global step 500: loss: 0.7288486361503601, lr: 0.001
2023-08-25 16:36:12 INFO     	 * (global step 550: loss: 0.6290831863880157, lr: 0.001
2023-08-25 16:37:56 INFO     [epoch 0/15] average loss: 0.77, lr: 0.001
2023-08-25 16:37:56 INFO     saving model related files
2023-08-25 16:37:56 INFO     saving model
2023-08-25 16:37:57 INFO     saving tokenizer
2023-08-25 16:37:57 INFO     saving optimizer
2023-08-25 16:38:00 INFO     remove old optimizer files
2023-08-25 16:38:35 INFO     	 * (global step 600: loss: 0.6017974615097046, lr: 0.001
2023-08-25 16:40:55 INFO     	 * (global step 650: loss: 0.6039931178092957, lr: 0.001
2023-08-25 16:43:13 INFO     	 * (global step 700: loss: 0.5816770195960999, lr: 0.001
2023-08-25 16:45:32 INFO     	 * (global step 750: loss: 0.6185030341148376, lr: 0.001
2023-08-25 16:47:51 INFO     	 * (global step 800: loss: 0.6231783628463745, lr: 0.001
2023-08-25 16:50:10 INFO     	 * (global step 850: loss: 0.6113907992839813, lr: 0.001
2023-08-25 16:52:29 INFO     	 * (global step 900: loss: 0.5992139577865601, lr: 0.001
2023-08-25 16:54:48 INFO     	 * (global step 950: loss: 0.5927358865737915, lr: 0.001
